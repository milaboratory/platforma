ll := import(":ll")
exp := import(":pt.expression")
util := import(":pt.util")
assets := import(":assets")
smart := import(":smart")
exec := import(":exec")
json := import("json")
text := import("text")
slices := import(":slices")
execConstants := import(":exec.constants")
pframes := import(":pframes")
pframesBuilder := import(":pframes.builder")
pframesConstants := import(":pframes.constants")
pframesUtil := import(":pframes.util")
pframesSpec := import(":pframes.spec")
render := import(":render")
maps := import(":maps")

/**
 * This library provides a high-level API, inspired by Polars DataFrames, for building and executing
 * PTabler data processing workflows within the Platforma SDK using Tengo.
 *
 * It allows users to define a sequence of data transformation steps (e.g., reading files,
 * selecting columns, filtering rows, performing aggregations, and window operations)
 * and then run them as a PTabler pipeline.
 *
 * @example
 *   pt := import("@platforma-sdk/workflow-tengo:pt")
 *   file := import("@platforma-sdk/workflow-tengo:file")
 *
 *   // Assuming 'inputs.myTsvFile' is a resource reference to a TSV file
 *   // and self.defineOutputs("outputResult") has been called.
 *
 *   wf := pt.workflow()
 *
 *   // Read a TSV file into a DataFrame
 *   df := wf.frame(inputs.myTsvFile, {xsvType: "tsv"})
 *
 *   // Example: Select column 'colA' and calculate its square root aliased as 'colA_sqrt'
 *   processedDf := df.select(
 *       pt.col("colA"),
 *       pt.col("colA").sqrt().alias("colA_sqrt")
 *   )
 *
 *   // Save the processed DataFrame to a file named "result.tsv"
 *   processedDf.save("result.tsv")
 *
 *   // Run the defined workflow
 *   ptablerResult := wf.run()
 *
 *   // Export the output file
 *   // return { outputResult: file.exportFile(ptablerResult.getFile("result.tsv")) }
 */


_newDataFrame := undefined
_newDataFrameGroupBy := undefined

workflowCounter := 0


_validateColumn := func(name, columnStruct) {
	ll.assert(is_string(name), "column name must be a string")
	ll.assert(is_map(columnStruct), "columnStruct must be a map")
	ll.assert(!is_undefined(columnStruct.spec), "columnStruct must have 'spec' field defined")
	ll.assert(!is_undefined(columnStruct.data), "columnStruct must have 'data' field defined")
}

_newPEntry := func(columnsByName, joinTree) {
	return {
		_getColumnsMap: func() {
			return columnsByName
		},
		_getJoinTree: func() {
			return joinTree
		}
	}
}

_isPEntry := func(entry) {
	return is_map(entry) && is_callable(entry._getColumnsMap) && is_callable(entry._getJoinTree);
}

slicedColumnCounter := 0

p := ll.toStrict({
	/**
	 * Creates a new column join builder entry.
	 * @param name {string} - The name of the column.
	 * @param columnStruct { spec: json, data: resource } - The column object.
	 * @returns {object} - A join builder entry object.
	 */
	column: func(name, columnStruct) {
		_validateColumn(name, columnStruct)
		columnsByName := {}
		columnsByName[name] = columnStruct
		return _newPEntry(columnsByName, {
			type: "column",
			columnId: name
		})
	},
	/**
	 * Creates a new sliced column join builder entry.
	 * @param name {string} - The name of the column.
	 * @param columnMap { spec: json, data: resource } - The column object.
	 * @param axisFilters {array} - The array of axis filters.
	 * @returns {object} - A join builder entry object.
	 */
	slicedColumn: func(name, columnMap, axisFilters) {
		_validateColumn(name, columnMap)
		ll.assert(is_array(axisFilters), "axisFilters must be an array")
		
		slicedColumnCounter += 1
		oldName := "p_sliced_" + slicedColumnCounter

		columnsByName := {}
		columnsByName[oldName] = columnMap
		return _newPEntry(columnsByName, {
			type: "slicedColumn",
			columnId: oldName,
			newId: name,
			axisFilters: axisFilters
		})
	},
	/**
	 * Creates a new inner join builder entry.
	 * @param entries {array} - The array of join builder entry objects.
	 * @returns {object} - A join builder entry object.
	 */
	inner: func(...entries) {
		ll.assert(len(entries) > 0, "inner join requires at least one entry")
		
		mergedColMap := {}
		joinTreeEntries := []
		
		for entry in entries {
			ll.assert(_isPEntry(entry), "all entries must be PEntry objects")
			
			entryColMap := entry._getColumnsMap()
			entryJoinTree := entry._getJoinTree()
			
			for colName, colInfo in entryColMap {
				if !is_undefined(mergedColMap[colName]) {
					ll.panic("column name %s is already used, please pick a different name", colName)
				}
				mergedColMap[colName] = colInfo
			}
			
			joinTreeEntries = append(joinTreeEntries, entryJoinTree)
		}
		
		joinTree := {
			type: "inner",
			entries: joinTreeEntries
		}
		
		return _newPEntry(mergedColMap, joinTree)
	},
	/**
	 * Creates a new full join builder entry.
	 * @param entries {array} - The array of join builder entry objects.
	 * @returns {object} - A join builder entry object.
	 */
	full: func(...entries) {
		ll.assert(len(entries) > 0, "full join requires at least one entry")
		
		mergedColMap := {}
		joinTreeEntries := []
		
		for entry in entries {
			ll.assert(_isPEntry(entry), "all entries must be PEntry objects")
			
			entryColMap := entry._getColumnsMap()
			entryJoinTree := entry._getJoinTree()
			
			for colName, colInfo in entryColMap {
				if !is_undefined(mergedColMap[colName]) {
					ll.panic("column name %s is already used, please pick a different name", colName)
				}
				mergedColMap[colName] = colInfo
			}
			
			joinTreeEntries = append(joinTreeEntries, entryJoinTree)
		}
		
		joinTree := {
			type: "full",
			entries: joinTreeEntries
		}
		
		return _newPEntry(mergedColMap, joinTree)
	},
	/**
	 * Creates a new outer join builder entry.
	 * @param primary {object} - The primary join builder entry object.
	 * @param secondary {array} - The array of secondary join builder entry objects.
	 * @returns {object} - A join builder entry object.
	 */
	outer: func(primary, ...secondary) {
		ll.assert(_isPEntry(primary), "primary entry must be a PEntry object")
		ll.assert(len(secondary) > 0, "outer join requires at least one secondary entry")
		
		mergedColMap := {}
		
		primaryColMap := primary._getColumnsMap()
		primaryJoinTree := primary._getJoinTree()
		
		for colName, colInfo in primaryColMap {
			mergedColMap[colName] = colInfo
		}
		
		secondaryJoinTrees := []
		
		for entry in secondary {
			ll.assert(_isPEntry(entry), "all secondary entries must be PEntry objects")
			
			entryColMap := entry._getColumnsMap()
			entryJoinTree := entry._getJoinTree()
			
			for colName, colInfo in entryColMap {
				if !is_undefined(mergedColMap[colName]) {
					ll.panic("column name %s is already used, please pick a different name", colName)
				}
				mergedColMap[colName] = colInfo
			}
			
			secondaryJoinTrees = append(secondaryJoinTrees, entryJoinTree)
		}
		
		joinTree := {
			type: "outer",
			primary: primaryJoinTree,
			secondary: secondaryJoinTrees
		}
		
		return _newPEntry(mergedColMap, joinTree)
	}
})

// Helper function to detect file format from file name extension
_detectFormatFromFileName := func(fileName) {
	if text.has_suffix(fileName, ".csv") {
		return "csv"
	} else if text.has_suffix(fileName, ".tsv") {
		return "tsv"
	} else if text.has_suffix(fileName, ".ndjson") || text.has_suffix(fileName, ".jsonl") {
		return "ndjson"
	} else if text.has_suffix(fileName, ".parquet") {
		return "parquet"
	}
	return undefined // No format detected
}

/**
 * Creates a new workflow. Workflow serves as a main object, that allows to build a PTabler pipeline,
 * using convenient Polars DataFrames API.
 *
 * @returns {object} - The workflow object.
 */
workflow := func() {
	steps := []

	// { file?: ResourceRef, content?: string, name: string }[]
	inFiles := []
	// { [uniqueFrameName]: { columns: { [workflowUniqueColumnId]: { spec: json, data: resource } } } }
	inFrames := {}
	// string[]
	outFiles := []
	// string[]
	outContentFiles := []
	// { [frameName]: { [pColumnId]: pColumnSpec } }
	outFramesSpecs := {}
	// { [frameName]: sequentialFrameName }
	outFramesRename := {}

	anonymousFilesCounter := 0
	anonymousDFCounter := 0
	pcolumnCounter := 0
	pframeCounter := 0
	specDistillerSingleton := undefined

	cpu := undefined
	mem := undefined
	inputCache := undefined
	queue := execConstants.MEDIUM_QUEUE

	self := undefined

	id := workflowCounter
	workflowCounter += 1

	self = {
		_wfId: id,

		/**
		 * Adds a raw PTabler step to the workflow.
		 * This is an advanced feature for users who need to define steps not covered by the high-level API.
		 * @param step {map} - A map representing the PTabler step structure.
		 * @returns {object} - The workflow object, allowing for method chaining.
		 */
		addRawStep: func(step) {
			ll.assert(is_map(step), "step must be a map")
			ll.assert(is_string(step.type), "step.type must be a string discriminator")
			self._addStep(func(ctx) {
				return step
			})
			return self
		},

		_addStep: func(step) {
			ll.assert(is_callable(step), "step must be a function")
			steps = append(steps, step)
		},

		/**
		 * Creates a new DataFrame in the workflow by reading from an input source.
		 * The input can be a resource reference, a string containing file content, or a structured map
		 * specifying the file and its properties.
		 *
		 * The structured input format (`{ file: ResourceRef, xsvType: "csv"|"tsv", schema?: map }`) is primarily
		 * intended for compatibility with the output of `xsvFileBuilder.buildForPT()` from
		 * `@platforma-sdk/workflow-tengo:pframes.xsv-builder`.
		 *
		 * @param frameInput {object|string} - The input source. Can be:
		 *   - A file resource reference (e.g., `inputs.myFile`).
		 *   - A string containing the raw content of the file (e.g., "colA\\tcolB\\n1\\t2" or NDJSON content).
		 *   - A map with the following fields (i.e. result of `xsvFileBuilder.buildForPT()`):
		 *     - `file` {object}: A resource reference to the input file.
		 *     - `xsvType` {string}: The type of the file, either "csv" or "tsv".
		 *     - `schema` {map} (optional): A PTabler schema definition for the input file.
		 *   - A p-frame resource or frame-like map (acceptable by `pframes.util.pFrameToColumnsMap()`)
		 * @param ...optionsRaw {map} (optional) - A single map argument for additional options:
		 *   - `format` {string}: The file format ("csv", "tsv", or "ndjson"). Takes priority over `xsvType`.
		 *                        If not provided, auto-detects from file extension or uses `xsvType`.
		 *   - `xsvType` {string}: The type of the file ("csv" or "tsv"). For backward compatibility.
		 *                         Required if `frameInput` is direct content/reference and `format` is not provided.
		 *   - `id` {string} (optional): A specific ID to assign to this DataFrame. If not provided, an anonymous ID is generated.
		 *   - `fileName` {string} (optional): A specific name for the input file within PTabler. If not provided, an anonymous name is generated.
		 *   - `inferSchema` {boolean} (optional): Whether to infer the schema from the input file. Defaults to `true`. If `false`,
		 *                                        type inference is disabled, and types will rely on the `schema` field or PTabler's defaults.
		 *   - `nRows` {int} (optional): Stop reading after this many rows. If not specified, all rows will be read.
		 *   - `ignoreErrors` {boolean} (optional): Return null if parsing fails due to schema mismatches. Defaults to `false`.
		 * @returns {object} - A DataFrame object representing the loaded data.
		 * @example
		 *   // NDJSON from file reference
		 *   df := wf.frame(inputs.dataFile, {format: "ndjson"})
		 *
		 *   // NDJSON with row limit and schema override
		 *   df := wf.frame(inputs.largeFile, {
		 *     format: "ndjson",
		 *     nRows: 10000,
		 *     schema: [{column: "id", type: "Int64"}]
		 *   })
		 *
		 *   // From string content (CSV)
		 *   csvContent := "header1,header2\\nvalue1,value2"
		 *   df2 := wf.frame(csvContent, {format: "csv", id: "myCsvData"})
		 *
		 *   // From a structured input (e.g., using output from xsvFileBuilder)
		 *   // xsvBuilderOutput := xsv.xsvFileBuilder("tsv").add(...).buildForPT()
		 *   // df3 := wf.frame(xsvBuilderOutput) // xsvBuilderOutput contains {file, xsvType, schema}
		 */
		frame: func(frameInput, ...optionsRaw) {
			ll.assert(!is_undefined(frameInput), "frameInput must be provided")
			
			opts := {}
			if len(optionsRaw) > 0 {
				if len(optionsRaw) == 1 && is_map(optionsRaw[0]) {
					opts = optionsRaw[0]
				} else {
					ll.panic("frame options must be a single map argument")
				}
			}

			frameResource := undefined
			fileRef := undefined
			fileContent := undefined
			inputSchema := undefined
			inputXsvType := undefined
			finalDataFrameId := undefined
			finalFileName := undefined
			finalFormat := undefined
			inferSchemaOpt := undefined
			optionsSchema := undefined
			nRowsOpt := undefined
			ignoreErrorsOpt := undefined

			if is_map(frameInput) && !is_undefined(frameInput.file) {
				fileRef = frameInput.file
				inputXsvType = frameInput.xsvType
				// schema might be undefined in the input, which is fine
				if !is_undefined(frameInput.schema) {
					if !is_array(frameInput.schema) {
						ll.panic("frameInput.schema (for structural input) must be an array if provided. Got: %T", frameInput.schema)
					}
					inputSchema = frameInput.schema
				}

				finalFormat = inputXsvType
			} else if smart.isReference(frameInput) {
				frameInputResource := frameInput
				if smart.isField(frameInputResource) {
					frameInputResource = frameInputResource.getValue()
				}
				if frameInputResource.checkResourceType(pframesConstants.RTYPE_P_FRAME) {
					frameResource = frameInputResource
					finalFormat = "pframe"
				} else {
					// TODO: check if it's a file reference
					fileRef = frameInput
				}
			} else if is_string(frameInput) || is_bytes(frameInput) {
				fileContent = frameInput
			} else if pframesUtil.isFlatPfMap(frameInput) {
				frameResource = frameInput
				finalFormat = "pframe"
			} else if pframesUtil.isStructuredPfMap(frameInput) {
				pfBuilder := pframesBuilder.pFrameBuilder()
				for key, value in frameInput {
					pfBuilder.add(key, value.spec, value.data)
				}
				frameResource = pfBuilder.build()
				finalFormat = "pframe"
			} else if _isPEntry(frameInput) {
				finalFormat = "pentry"
			} else {
				ll.panic("unknown frame input type: %v", frameInput)
			}

			// Format resolution logic: format > xsvType > auto-detection
			if !is_undefined(opts.format) {
				if opts.format != "csv" && opts.format != "tsv" && opts.format != "ndjson" {
					ll.panic("format must be 'csv', 'tsv', or 'ndjson'. Got: %v", opts.format)
				}
				if !is_undefined(finalFormat) && finalFormat != opts.format {
					ll.panic("format and xsvType from structural input cannot be used together. Got: %v and %v", opts.format, finalFormat)
				}
				finalFormat = opts.format
			} else if !is_undefined(opts.xsvType) {
				if opts.xsvType != "csv" && opts.xsvType != "tsv" {
					ll.panic("xsvType must be 'csv' or 'tsv'. Got: %v", opts.xsvType)
				}
				if !is_undefined(finalFormat) && finalFormat != opts.xsvType {
					ll.panic("opts.xsvType and xsvType from structural input cannot be used together. Got: %v and %v", opts.xsvType, finalFormat)
				}
				finalFormat = opts.xsvType
			}

			if !is_undefined(opts.id) {
				finalDataFrameId = opts.id
			} else {
				finalDataFrameId = self._newAnonymousDataFrameId()
			}

			if !is_undefined(opts.fileName) {
				finalFileName = opts.fileName
				if is_undefined(finalFormat) {
					detectedFormat := _detectFormatFromFileName(finalFileName)
					if !is_undefined(detectedFormat) {
						finalFormat = detectedFormat
					}
				}
			} else {
				finalFileName = self._newAnonymousFileId(finalFormat)
			}

			// Final validation - format must be determined by now
			if is_undefined(finalFormat) {
				ll.panic("Unable to determine file format. Please specify 'format' or 'xsvType' option, or use a fileName with a recognizable extension (.csv, .tsv, .ndjson, .jsonl)")
			}

			if !is_undefined(opts.inferSchema) {
				if !is_bool(opts.inferSchema) {
					ll.panic("'inferSchema' option must be a boolean. Got: %T", opts.inferSchema)
				}
				inferSchemaOpt = opts.inferSchema
			}

			if !is_undefined(opts.schema) {
				if !is_array(opts.schema) {
					ll.panic("'schema' option (from options map) must be an array. Got: %T", opts.schema)
				}
				optionsSchema = opts.schema
			}

			if !is_undefined(opts.nRows) {
				if !is_int(opts.nRows) || opts.nRows < 0 {
					ll.panic("'nRows' option must be a non-negative integer. Got: %v", opts.nRows)
				}
				nRowsOpt = opts.nRows
			}

			if !is_undefined(opts.ignoreErrors) {
				if !is_bool(opts.ignoreErrors) {
					ll.panic("'ignoreErrors' option must be a boolean. Got: %T", opts.ignoreErrors)
				}
				ignoreErrorsOpt = opts.ignoreErrors
			}

			if !is_undefined(fileRef) {
				ll.assert(
					smart.isReference(fileRef),
					"frameInput.file (for structural input) or frameInput (for direct reference) must be a valid resource reference, got: %v",
					fileRef)
				inFiles = append(inFiles, { file: fileRef, name: finalFileName })
			} else if !is_undefined(fileContent) {
				ll.assert(is_string(fileContent), "frameInput (for direct content) must be a string, got: %v", fileContent)
				inFiles = append(inFiles, { content: fileContent, name: finalFileName })
			}

		// Generate appropriate step based on format
		step := undefined
		if finalFormat == "pentry" || finalFormat == "pframe" {
			if finalFormat == "pframe" {
				columnsMap := pframesUtil.pFrameToColumnsMap(frameResource)
				columnEntries := []
				for frameUniqueColumnId, columnInfo in columnsMap {
					columnEntry := p.column(frameUniqueColumnId, columnInfo)
					columnEntries = append(columnEntries, columnEntry)
				}
				frameInput = p.full(columnEntries...)
			}

			columnsMap := frameInput._getColumnsMap()
			src := frameInput._getJoinTree()

			// Build translation and rename mappings
			translation := {}
			rename := {}
			renamedColumnsMap := {}

			for frameUniqueColumnId, columnInfo in columnsMap {
				workflowUniqueColumnId := self._newUniqueColumnId()

				translation[workflowUniqueColumnId] = frameUniqueColumnId
				rename[frameUniqueColumnId] = workflowUniqueColumnId
				renamedColumnsMap[workflowUniqueColumnId] = columnInfo
			}

			// Create request and rename columns
			request := {
				src: src,
				filters: []
			}
			renamedRequest := util.renameJoinColumns(request, rename)

			// Register frame and create step
			ll.assert(is_undefined(inFrames[finalDataFrameId]),
				"frame with name '%s' already added", finalDataFrameId)
			inFrames[finalDataFrameId] = {
				columns: renamedColumnsMap
			}

			step = {
				type: "read_frame",
				name: finalDataFrameId,
				request: renamedRequest,
				translation: translation
			}
		} else if finalFormat == "parquet" {
				step = {
					type: "read_parquet",
					file: finalFileName,
					name: finalDataFrameId
				}
			} else if finalFormat == "ndjson" {
				step = {
					type: "read_ndjson",
					file: finalFileName,
					name: finalDataFrameId
				}
			} else { // csv or tsv
				step = {
					type: "read_csv",
					file: finalFileName,
					name: finalDataFrameId
				}

				if finalFormat == "csv" {
					step.delimiter = ","
				} else if finalFormat == "tsv" {
					step.delimiter = "\t"
				}
			}

			// Determine final schema for PTabler: optionsSchema (from opts) overrides inputSchema (from structural input)
			finalSchemaToUse := undefined
			if !is_undefined(optionsSchema) {
				finalSchemaToUse = optionsSchema
			} else {
				finalSchemaToUse = inputSchema
			}

			// Add common optional fields to the step
			if !is_undefined(finalSchemaToUse) {
				step.schema = finalSchemaToUse
			}

			if !is_undefined(inferSchemaOpt) && inferSchemaOpt == false {
				step.inferSchema = false
			}

			if !is_undefined(ignoreErrorsOpt) && ignoreErrorsOpt == true {
				step.ignoreErrors = true
			}

			if !is_undefined(nRowsOpt) {
				step.nRows = nRowsOpt
			}

			self.addRawStep(step)
			return _newDataFrame(self, finalDataFrameId)
		},

		/**
		 * Create a frame from a column bundle. This allows constructing a DataFrame by joining multiple columns,
		 * potentially with filters applied to axes.
		 * 
		 * @param bundle {object} - A column bundle object, typically created using `wf.createPBundleBuilder`.
		 * @param options {map} - A map with the following fields:
		 *   - `axes` {array} - An array of axis definitions. Each entry should be a map with:
		 *   - `columns` {array} - An array of column definitions. Each entry should be a map with:
		 * @returns {object} - A DataFrame object constructed from the column bundle.
		 */
		frameFromColumnBundle: func(bundle, options) {
			util.validateParamsForFrameFromColumnBundle(options)

			decodeColumn := func(columnId) {
				decoded := undefined
				if is_string(columnId) {
					decoded = json.decode(columnId)
				} else if is_map(columnId) {
					decoded = columnId
				} else {
					ll.panic("Invalid column ID type. Expected string or object.")
				}
				return decoded
			}
			isFilteredColumn := func(columnId) {
				decoded := decodeColumn(columnId)
				return is_map(decoded) && !is_undefined(decoded.source) && !is_undefined(decoded.axisFilters)
			}

			axes := options.axes
			columns := options.columns
			tsvBuilder := pframes.tsvFileBuilder()
			axesHeaders := []

			for idx, axis in axes {
				tsvBuilder.setAxisHeader(axis.spec, axis.column)
				axesHeaders = append(axesHeaders, axis.column)
			}

			filteredColumns := []
			for column in columns {
				if (isFilteredColumn(column)) {
					filteredColumns = append(filteredColumns, column)
				} else {
					tsvBuilder.add(bundle.getColumn(column))
				}
			}
			
			mainDF := self.frame(tsvBuilder.buildForPT())

			for column in filteredColumns {
				decoded := decodeColumn(column)
				source := decoded.source
				axisFilters := decoded.axisFilters

				tsv := pframes.tsvFileBuilder()
				for axis in axes {
					tsv.setAxisHeader(axis.spec, axis.column)
				}
				tsv.add(bundle.getColumn(source), { header: column })
				df := self.frame(tsv.buildForPT())
				joinColumns := slices.map(axes, func(axis) { return axis.column })
				for axisFilter in axisFilters {
					id := axesHeaders[axisFilter[0]]
					value := axisFilter[1]
					df = df.filter(exp.col(id).eq(exp.lit(value))).withoutColumns(id)
					joinColumns = slices.filter(joinColumns, func(c) { return c != id } )
				}

				mainDF = mainDF.join(df, { on: joinColumns })
			}

			return mainDF
		},

		_newAnonymousDataFrameId: func() {
			anonymousDFCounter += 1
			return "anonymous_" + string(anonymousDFCounter)
		},
		_newAnonymousFileId: func(extension) {
			anonymousFilesCounter += 1
			return "anonymous_" + string(anonymousFilesCounter) + "." + extension
		},
		_newUniqueColumnId: func() {
			pcolumnCounter += 1
			return "pcolumn_" + string(pcolumnCounter)
		},
		_newUniquePFrameName: func() {
			pframeCounter += 1
			return "pframe_" + string(pframeCounter)
		},
		_saveFile: func(name) {
			outFiles = append(outFiles, name)
		},
		_saveFileContent: func(name) {
			outContentFiles = append(outContentFiles, name)
		},
		_saveFrameSpecs: func(name, rename, specs) {
			ll.assert(
				outFramesSpecs[name] == undefined,
				"frame with name '%s' already saved, please pick a different name",
				name
			)
			outFramesSpecs[name] = specs
			outFramesRename[name] = rename
		},
		_getSpecDistiller: func() {
			if is_undefined(specDistillerSingleton) {
				rawColumnSpecs := []
				for _, columns in inFrames {
					for columnId, columnInfo in columns {
						rawColumnSpecs = append(rawColumnSpecs, columnInfo.spec)
					}
				}
				specDistillerSingleton = pframesSpec.createSpecDistiller(rawColumnSpecs)
			}
			return specDistillerSingleton
		},
		_getDistilledAxesSpecs: func() {
			specDistiller := self._getSpecDistiller()
			distilledAxesSpecs := {}
			for _, frame in inFrames {
				for _, columnInfo in frame.columns {
					for axis in columnInfo.spec.axesSpec {
						distilledAxisSpec := specDistiller.distill(axis)
						if is_undefined(distilledAxesSpecs[distilledAxisSpec.name]) {
							distilledAxesSpecs[distilledAxisSpec.name] = []
						}
						found := false
						for _, appendedSpec in distilledAxesSpecs[distilledAxisSpec.name] {
							if distilledAxisSpec.type != appendedSpec.type {
								continue
							}
							if is_undefined(distilledAxisSpec.domain) && is_undefined(appendedSpec.domain) {
								found = true
								break
							}
							if is_undefined(distilledAxisSpec.domain) || is_undefined(appendedSpec.domain) {
								continue
							}
							if len(distilledAxisSpec.domain) != len(appendedSpec.domain) {
								continue
							}
							for domainKey, domainValue in distilledAxisSpec.domain {
								if appendedSpec.domain[domainKey] != domainValue {
									continue
								}
							}
							found = true
							break
						}
						if !found {
							distilledAxesSpecs[distilledAxisSpec.name] = append(distilledAxesSpecs[distilledAxisSpec.name], distilledAxisSpec)
						}
					}
				}
			}
			return distilledAxesSpecs
		},

		/**
		 * Execute pt in the 'heavy' queue.
		 */
		inHeavyQueue: func() {
			queue = execConstants.HEAVY_QUEUE
			return self
		},

		/**
		 * Execute pt in the 'medium' queue.
		 */
		inMediumQueue: func() {
			queue = execConstants.MEDIUM_QUEUE
			return self
		},

		/**
		 * Execute pt in the 'light' queue.
		 */
		inLightQueue: func() {
			queue = execConstants.LIGHT_QUEUE
			return self
		},

		/**
		 * Execute pt in the 'ui-tasks' queue.
		 */
		inUiQueue: func() {
			queue = execConstants.UI_TASKS_QUEUE
			return self
		},

		/**
		 * Sets the number of CPUs to request from the underlying executor (i.e. Google/AWS Batch, PBS, Slurm, etc.).
		 *
		 * @param amount: number - number of cores requested for command.
		 */
		cpu: func(amount) {
			cpu = amount
			return self
		},

		/**
		 * Sets the amount of RAM to request from the underlying executor (i.e. Google/AWS Batch, PBS, Slurm, etc.).
		 *
		 * @param amount: number | string - amount of RAM in bytes or string with size suffix (case-insensitive):
		 *                                     K,  KB,  M,  MB,  G,  GB for base-10 sizes (powers of 1000)
		 *                                    Ki, KiB, Mi, MiB, Gi, GiB for base-2 sizes (powers of 1024)
		 *                                  when operating with bytes, you may use 'units' package for convenience:
		 *                                    120 * units.GiB
		 *
		 * @return builder
		 */
		mem: func(amount) {
			mem = amount
			return self
		},

		/**
		 * Sets the cache duration for the execution's inputs.
		 *
		 * This is useful for an execution that is part of a sequence that might be re-rendered soon.
		 * Caching allows the inputs to be reused by the recovery mechanism, avoiding the need to
		 * recalculate them. A duration of one minute is recommended.
		 *
		 * @param time: duration - the cache time from 'times' library.
		 */
		cacheInputs: func(time) {
			ll.assert(
				is_int(time),
				"input cache time must be an integer. " +
					"Did you forget to import a standard tengo library 'times'?")
			inputCache = time
			return self
		},

		/**
		 * Executes the defined PTabler workflow.
		 * This compiles all the added steps and input/output file configurations,
		 * then runs the PTabler command.
		 * @returns {object} - A result object from `exec.run()`, typically providing access to output files via `getFile()` or `getFileContent()`.
		 */
		run: func() {
			workflowRunTpl := assets.importTemplate(":pt.workflow-run")

			specDistiller := self._getSpecDistiller()
			distilledAxesSpecs := self._getDistilledAxesSpecs()
			ctx := {
				specDistiller: specDistiller,
				distilledAxesSpecs: distilledAxesSpecs
			}
			workflowJsonStruct := {
				workflow: slices.map(steps, func(step) {
					return step(ctx)
				})
			}

			inFilesMap := {}
			inFilesContentMap := {}
			for inFile in inFiles {
				if !is_undefined(inFile.file) {
					inFilesMap[inFile.name] = inFile.file
				} else if !is_undefined(inFile.content) {
					inFilesContentMap[inFile.name] = inFile.content
				} else {
					ll.panic("inFile must have either file or content, got: %v", inFile)
				}
			}
			
			outFrames := {}
			for frameName, specsById in outFramesSpecs {
				columnIds := []
				for columnId, _ in specsById {
					columnIds = append(columnIds, columnId)
				}
				outFrames[outFramesRename[frameName]] = columnIds;
			}

			inFramesDistilled := maps.mapValues(inFrames, func(frame) {
				return {
					columns: maps.mapValues(frame.columns, func(columnInfo) {
						return {
							spec: specDistiller.distill(columnInfo.spec),
							data: columnInfo.data
						}
					})
				}
			})

			runInputs := {
				workflowJsonStruct: workflowJsonStruct,
				inFiles: inFilesMap,
				inFilesContent: inFilesContentMap,
				inFrames: inFramesDistilled,
				outFiles: outFiles,
				outFilesContent: outContentFiles,
				outFrames: outFrames
			}
			runMetaInputs := {
				queue: queue,
				cpu: cpu,
				mem: mem,
				inputCache: inputCache
			}
			ptResult := render.create(workflowRunTpl, runInputs, {
				metaInputs: runMetaInputs
			});

			self := undefined
			self = ll.toStrict({
				/**
				 * Retrieves a file.
				 *
				 * @param fileName: string
				 * @return field - a reference to the file resource.
				 */
				getFile: func(fileName) {
					return ptResult.resolveOutput(["outFiles", fileName])
				},

				/**
				 * Retrieves the content of a file.
				 *
				 * @param fileName: string
				 * @return field - a reference to the file content resource.
				 */
				getFileContent: func(fileName) {
					return ptResult.resolveOutput(["outFilesContent", fileName])
				},

				/**
				 * Retrieves exported frame.
				 *
				 * @param frameName: string - name of the frame to retrieve
				 * @return frame constructed with `pframes.pFrameBuilder()`
				 */
				getFrameDirect: func(frameName) {
					specs := outFramesSpecs[frameName]
					ll.assert(!is_undefined(specs), "saveFrameDirect was not called for frame '%s'", frameName)

					pf := pframes.pFrameBuilder()
					for columnId, columnSpec in specs {
						columnData := ptResult.resolveOutput(["outFrames", outFramesRename[frameName], columnId])
						pf.add(columnId, columnSpec, columnData)
					}
					return pf.build()
				}
			})
			return self
		}
	}

    return ll.toStrict(self)
}

_newDataFrameGroupBy = func(parentWorkflow, dfName, groupByExpressions) {
	self := undefined
	self = ll.toStrict({
		/**
		 * Performs aggregations on the grouped data.
		 * Takes one or more aggregation expressions created by applying aggregation functions (e.g., `sum()`, `mean()`)
		 * to column expressions (e.g., `pt.col("my_column").sum()`).
		 * Each aggregation expression should typically have an alias defined using `.alias("new_column_name")`.
		 *
		 * @param ...aggExpressions {object} - One or more aggregation expression objects.
		 * @returns {object} - A new DataFrame object with the aggregated results.
		 * @example
		 *   // Assuming df is a DataFrame
		 *   groupedDf := df.groupBy("category")
		 *                  .agg(
		 *                      pt.col("value").sum().alias("total_value"),
		 *                      pt.col("score").mean().alias("average_score")
		 *                  )
		 */
		agg: func(...aggExpressions) {
			ll.assert(len(aggExpressions) > 0, "agg method requires at least one aggregation expression.")

			outputDfName := parentWorkflow._newAnonymousDataFrameId()
			parentWorkflow._addStep(func(ctx) {
				aggregations := slices.map(aggExpressions, func(aggExpr) {
					if !exp._isAggregation(aggExpr) {
						ll.panic("Invalid argument to agg: Expected an aggregation expression object, got %v", aggExpr)
					}
					return aggExpr.getAggregation(ctx)
				})
				
				return {
					type: "aggregate",
					inputTable: dfName,
					outputTable: outputDfName,
					groupBy: exp._mapToExpressionStructList(groupByExpressions, "col", ctx),
					aggregations: aggregations
				}
			})
			return _newDataFrame(parentWorkflow, outputDfName)
		}
	})
	return self
}

_newDataFrame = func(parentWorkflow, dfName) {
	_mapExprsToStepCols := func(exprs, methodName, ctx) {
		ll.assert(len(exprs) > 0, methodName + " requires at least one expression argument.")
		cols := []
		for expr in exprs {
			if !exp._isExpression(expr) {
				ll.panic("Invalid argument to " + methodName + ": Expected an expression object, got %v", expr)
			}
			cols = append(cols, {
				name: expr.getAlias(),
				expression: expr._getExpression(ctx)
			})
		}
		return cols
	}

	_mapExprsToStepColsAllowingStrings := func(exprs, methodName, ctx) {
		ll.assert(len(exprs) > 0, methodName + " requires at least one expression argument.")
		cols := []
		for expr in exprs {
			if is_string(expr) {
				// For strings, interpret as column name
				colExpr := exp.col(expr)
				cols = append(cols, {
					name: colExpr.getAlias(),
					expression: colExpr._getExpression(ctx)
				})
			} else if exp._isExpression(expr) {
				cols = append(cols, {
					name: expr.getAlias(),
					expression: expr._getExpression(ctx)
				})
			} else {
				ll.panic("Invalid argument to " + methodName + ": Expected an expression object or string column name, got %v", expr)
			}
		}
		return cols
	}

	_addSaveStep := func(outputFile, ...options) {
		opts := {}
		if len(options) > 0 {
			opts = options[0]
		}

		finalFormat := undefined

		// Format resolution: explicit format > xsvType > auto-detection from extension
		if !is_undefined(opts.format) {
			if opts.format != "csv" && opts.format != "tsv" && opts.format != "ndjson" {
				ll.panic("format must be 'csv', 'tsv', or 'ndjson'. Got: %v", opts.format)
			}
			finalFormat = opts.format
		} else if !is_undefined(opts.xsvType) {
			if opts.xsvType != "csv" && opts.xsvType != "tsv" {
				ll.panic("xsvType must be 'csv' or 'tsv'. Got: %v", opts.xsvType)
			}
			finalFormat = opts.xsvType
		} else {
			// Auto-detect from file extension
			detectedFormat := _detectFormatFromFileName(outputFile)
			if !is_undefined(detectedFormat) {
				finalFormat = detectedFormat
			}
		}

		if is_undefined(finalFormat) {
			ll.panic("Unable to determine output format from file extension. Please specify 'format' or 'xsvType' option, or use a file name with a recognizable extension (.csv, .tsv, .ndjson, .jsonl)")
		}

		step := undefined
		if finalFormat == "parquet" {
			step = {
				type: "write_parquet",
				table: dfName,
				file: outputFile
			}
		} else if finalFormat == "ndjson" {
			step = {
				type: "write_ndjson",
				table: dfName,
				file: outputFile
			}
		} else { // csv or tsv
			delimiter := undefined
			if finalFormat == "csv" {
				delimiter = ","
			} else if finalFormat == "tsv" {
				delimiter = "\t"
			}

			step = {
				type: "write_csv",
				table: dfName,
				file: outputFile,
				delimiter: delimiter
			}
		}

		if !is_undefined(opts.columns) {
			step.columns = opts.columns
		}

		parentWorkflow.addRawStep(step)
	}

	self := undefined

	self = ll.toStrict({
		_getWorkflow: func() {
			return parentWorkflow
		},
		_getDfName: func() {
			return dfName
		},
		/**
		 * Adds or replaces columns in the DataFrame.
		 * If a column with the alias specified in an expression already exists, it will be replaced.
		 * If it doesn't exist, a new column is added.
		 *
		 * @param ...expressions {object} - One or more expression objects. Each expression should define an alias
		 *                                   using `.alias("new_column_name")` which will be the name of the resulting column.
		 * @returns {object} - A new DataFrame object with the modified columns.
		 * @example
		 *   dfWithNewCols := df.withColumns(
		 *       pt.col("price").multiply(pt.lit(1.2)).alias("price_with_tax"), // New column
		 *       pt.col("name").strToUpper().alias("name_uppercase")      // New column or replaces 'name_uppercase'
		 *   )
		 */
		withColumns: func(...expressions) {
			outputDfName := parentWorkflow._newAnonymousDataFrameId()
			parentWorkflow._addStep(func(ctx) {
				stepCols := _mapExprsToStepCols(expressions, "withColumns", ctx)
				return {
					type: "with_columns",
					inputTable: dfName,
					outputTable: outputDfName,
					columns: stepCols
				}
			})
			return _newDataFrame(parentWorkflow, outputDfName)
		},

		/**
		 * Selects a subset of columns, potentially transforming them or creating new ones.
		 * Only the columns resulting from the provided expressions will be present in the new DataFrame.
		 *
		 * @param ...expressions {object|string} - One or more expression objects or string column names.
		 *                                          Expression objects should define an alias using `.alias("column_name")`.
		 *                                          String arguments are interpreted as column names.
		 * @returns {object} - A new DataFrame object containing only the selected/transformed columns.
		 * @example
		 *   selectedDf := df.select(
		 *       "id",  // String column name
		 *       pt.col("value").plus(pt.lit(10)).alias("value_plus_10")  // Expression object
		 *   )
		 */
		select: func(...expressions) {
			outputDfName := parentWorkflow._newAnonymousDataFrameId()
			parentWorkflow._addStep(func(ctx) {
				stepCols := _mapExprsToStepColsAllowingStrings(expressions, "select", ctx)
				return {
					type: "select",
					inputTable: dfName,
					outputTable: outputDfName,
					columns: stepCols
				}
			})
			return _newDataFrame(parentWorkflow, outputDfName)
		},

		/**
		 * Adds new columns to the DataFrame. This operation modifies the DataFrame in-place.
		 * If a column with the same alias already exists, PTabler's behavior might vary (e.g. error or overwrite);
		 * it's generally safer to use `withColumns` if overwriting is intended and predictable behavior is desired.
		 *
		 * @param ...expressions {object} - One or more expression objects. Each expression should define an alias
		 *                                   using `.alias("new_column_name")`.
		 * @returns {object} - The same DataFrame object, now with the added columns (for chaining).
		 * @example
		 *   df.addColumns(
		 *       pt.col("colA").plus(pt.col("colB")).alias("sum_A_B")
		 *   ) // df is modified
		 */
		addColumns: func(...expressions) {
			parentWorkflow._addStep(func(ctx) {
				stepCols := _mapExprsToStepCols(expressions, "addColumns", ctx)
				return {
					type: "add_columns",
					table: dfName,
					columns: stepCols
				}
			})
			return self
		},

		/**
		 * Saves the DataFrame to a file.
		 * The file format is inferred from the `outputFile` extension if not specified in options.
		 * Supports CSV, TSV, and NDJSON formats.
		 *
		 * @param outputFile {string} - The path/name of the file to save the DataFrame to (e.g., "result.csv", "data/output.tsv", "output.ndjson").
		 * @param ...options {map} (optional) - A single map argument for additional options:
		 *   - `format` {string} (optional): Specify "csv", "tsv", or "ndjson". Takes priority over `xsvType` and overrides file extension inference.
		 *   - `xsvType` {string} (optional): Specify "csv" or "tsv". For backward compatibility. Overridden by `format` if both provided.
		 *   - `columns` {array} (optional): An array of column name strings to include in the output. If not provided, all columns are saved.
		 * @returns {object} - The same DataFrame object (for chaining).
		 * @example
		 *   // Auto-detection from extension
		 *   df.save("my_data.tsv")
		 *   df.save("result.ndjson")
		 *
		 *   // Explicit format override
		 *   df.save("data.txt", {format: "ndjson"})
		 *
		 *   // Column selection
		 *   df.save("specific_cols.csv", {columns: ["id", "name", "value"]})
		 *   df.save("subset.jsonl", {columns: ["id", "score"]})
		 *
		 *   // Backward compatibility
		 *   df.save("legacy.csv", {xsvType: "csv"})
		 */
		save: func(outputFile, ...options) {
			_addSaveStep(outputFile, options...)

			parentWorkflow._saveFile(outputFile)

			return self
		},

		/**
		 * Saves the DataFrame to a file and registers it to have its content accessible after `wf.run()`.
		 * Useful when the content of the saved file needs to be read directly in the Tengo script
		 * after the PTabler execution, rather than just being an output artifact.
		 * The file format is inferred from the `outputFile` extension if not specified in options.
		 * Supports CSV, TSV, and NDJSON formats.
		 *
		 * @param outputFile {string} - The path/name of the file.
		 * @param ...options {map} (optional) - A single map argument for additional options:
		 *   - `format` {string} (optional): Specify "csv", "tsv", or "ndjson". Takes priority over `xsvType` and overrides file extension inference.
		 *   - `xsvType` {string} (optional): Specify "csv" or "tsv". For backward compatibility. Overridden by `format` if both provided.
		 *   - `columns` {array} (optional): An array of column name strings to include.
		 * @returns {object} - The same DataFrame object (for chaining).
		 * @example
		 *   // Content will be available via ptablerResult.getFileContent("report.csv")
		 *   df.saveContent("report.csv")
		 *
		 *   // NDJSON content accessible after execution
		 *   df.saveContent("data.ndjson")
		 *
		 *   // Explicit format override
		 *   df.saveContent("output.txt", {format: "ndjson"})
		 */
		saveContent: func(outputFile, ...options) {
			_addSaveStep(outputFile, options...)

			parentWorkflow._saveFileContent(outputFile)

			return self
		},

		/**
		 * Saves the DataFrame as a PFrame and registers it to have its content accessible after `wf.run()`.
		 * 
		 * @param frameName {string} - The name of the PFrame.
		 * @param params {object} - PFrame creation parameters.
		 * @returns {void} - No other operation could be chained after this call.
		 * @example
		 *   // Save DataFrame as a PFrame with name "output_frame" and first axis partitioned
		 *   df.saveFrameDirect("output_frame", {
		 *     axes: [{name: "clusterId", type: "Long"}, {name: "name", type: "String"}],
		 *     columns: [{name: "value", type: "Double"}],
		 *     partitionKeyLength: 1
		 *   })
		 */
		saveFrameDirect: func(frameName, params) {
			ll.assert(len(frameName) > 0, "frameName must be a non-empty string")
			util.validateParamsForSaveFrameDirect(params)

			specs := util.makeFrameSpecs(params)
			sequentialFrameName := parentWorkflow._newUniquePFrameName()
			parentWorkflow._saveFrameSpecs(frameName, sequentialFrameName, specs)

			step := util.makeWriteFrameStep(sequentialFrameName, dfName, params)
			parentWorkflow.addRawStep(step)
		},

		/**
		 * Groups the DataFrame by one or more columns or expressions.
		 * This is followed by an `.agg()` call to perform aggregations on the groups.
		 *
		 * @param ...expressions {string|object} - One or more column names (strings) or expression objects to group by.
		 * @returns {object} - A DataFrameGroupBy object, which has an `agg()` method.
		 * @example
		 *   grouped := df.groupBy("category", pt.col("year").alias("sale_year"))
		 *   // now call .agg() on 'grouped'
		 */
		groupBy: func(...expressions) {
			ll.assert(len(expressions) > 0, "groupBy requires at least one expression argument.")
			// Validate expressions - they should be resolvable to column names or be expressions
			// _mapToExpressionStructList in agg step will do the final mapping
			for expr in expressions {
				if !is_string(expr) && !exp._isExpression(expr) {
					ll.panic("Invalid argument to groupBy: Expected a column name (string) or an expression object, got %v", expr)
				}
			}
			return _newDataFrameGroupBy(parentWorkflow, dfName, expressions)
		},

		/**
		 * Filters rows in the DataFrame based on one or more predicate expressions.
		 * If multiple predicates are provided, they are combined with a logical AND.
		 *
		 * @param ...predicates {object} - One or more boolean expression objects.
		 * @returns {object} - A new DataFrame object containing only the rows that satisfy the condition(s).
		 * @example
		 *   filteredDf := df.filter(
		 *       pt.col("value").gt(pt.lit(100)),
		 *       pt.col("category").eq(pt.lit("A"))
		 *   )
		 */
		filter: func(...predicates) {
			ll.assert(len(predicates) > 0, "filter method requires at least one predicate expression.")

			allConditions := []
			for p in predicates {
				if !exp._isExpression(p) {
					ll.panic("Invalid argument in filter: Expected an expression object, got %v", p)
				}
				allConditions = append(allConditions, p)
			}

			condition := undefined
			if len(allConditions) == 1 {
				condition = allConditions[0]
			} else {
				condition = exp.and(allConditions...)
			}

			outputDfName := parentWorkflow._newAnonymousDataFrameId()
			parentWorkflow._addStep(func(ctx) {
				return {
					type: "filter",
					inputTable: dfName,
					outputTable: outputDfName,
					condition: condition._getExpression(ctx)
				}
			})
			return _newDataFrame(parentWorkflow, outputDfName)
		},

		/**
		 * Limits the number of rows in the DataFrame.
		 *
		 * @param nRows {number} - The number of rows to limit the DataFrame to.
		 * @returns {object} - A new DataFrame object containing no more than nRows rows.
		 * @example
		 *   limitedDf := df.limit(100)
		 */
		limit: func(nRows) {
			ll.assert(is_int(nRows) && nRows > 0, "limit argument must be a positive number.")

			outputDfName := parentWorkflow._newAnonymousDataFrameId()
			parentWorkflow.addRawStep({
				type: "limit",
				inputTable: dfName,
				outputTable: outputDfName,
				n: nRows
			})
			return _newDataFrame(parentWorkflow, outputDfName)
		},

		/**
		 * Sorts the DataFrame by one or more columns or expressions.
		 * Sorting is always stable (maintains relative order of equivalent rows).
		 *
		 * @param by {array} - An array of sort keys. Each key can be a column name (string) or an expression object.
		 * @param ...optionsRaw {map} (optional) - A single map argument for additional options:
		 *   - `descending` {boolean|array} (optional): If a boolean, applies to all sort keys. If an array of booleans,
		 *     it must match the length of `by` and specifies descending order for each key individually. Defaults to false (ascending).
		 *   - `nulls_last` {boolean|array} (optional): If a boolean, applies to all sort keys. If an array of booleans,
		 *     it must match the length of `by`. If true, nulls are placed after non-nulls; if false, before. PTabler's default may vary.
		 * @returns {object} - A new DataFrame object with sorted rows.
		 * @example
		 *   // Sort by 'score' descending, then 'name' ascending
		 *   sortedDf := df.sort(
		 *       [pt.col("score"), "name"],
		 *       {descending: [true, false]}
		 *   )
		 *
		 *   // Sort by 'date', nulls first (always stable)
		 *   sortedDf := df.sort(
		 *       ["date"],
		 *       {nulls_last: false}
		 *   )
		 */
		sort: func(by, ...optionsRaw) {
			ll.assert(is_array(by), "First argument to sort must be an array of sort keys (column names or expressions).")
			ll.assert(len(by) > 0, "Sort keys array cannot be empty.")

			opts := {}
			if len(optionsRaw) > 0 {
				ll.assert(len(optionsRaw) == 1, "sort expects at most one options map argument after the sort keys array.")
				if !is_map(optionsRaw[0]) {
					ll.panic("Second argument to sort (if provided) must be an options map. Got: %T", optionsRaw[0])
				}
				opts = optionsRaw[0]
			}

			descendingOpt := opts.descending
			nullsLastOpt := opts.nulls_last

			if !is_undefined(descendingOpt) && !is_bool(descendingOpt) && !is_array(descendingOpt) {
				ll.panic("sort 'descending' option must be a boolean or an array of booleans. Got: %T", descendingOpt)
			}
			if is_array(descendingOpt) && len(descendingOpt) != len(by) {
				ll.panic("sort 'descending' array length (%d) must match the number of sort keys (%d).", len(descendingOpt), len(by))
			}

			if !is_undefined(nullsLastOpt) && !is_bool(nullsLastOpt) && !is_array(nullsLastOpt) {
				ll.panic("sort 'nulls_last' option must be a boolean or an array of booleans. Got: %T", nullsLastOpt)
			}
			if is_array(nullsLastOpt) && len(nullsLastOpt) != len(by) {
				ll.panic("sort 'nulls_last' array length (%d) must match the number of sort keys (%d).", len(nullsLastOpt), len(by))
			}

			outputDfName := parentWorkflow._newAnonymousDataFrameId()

			parentWorkflow._addStep(func(ctx) {
				sortDirectives := []
				for i, item in by {
					directive := { value: undefined }

					if is_string(item) {
						directive.value = exp.col(item)._getExpression(ctx)
					} else if exp._isExpression(item) {
						directive.value = item._getExpression(ctx)
					} else {
						ll.panic("Invalid sort key at index %d in array: Expected a column name (string) or an expression object, got %T", i, item)
					}

					// Handle 'descending'
					itemDesc := false // Polars default
					if is_array(descendingOpt) {
						val := descendingOpt[i]
						ll.assert(is_bool(val), "Elements of 'descending' array (at index %d) must be booleans. Got: %T", i, val)
						itemDesc = val
					} else if is_bool(descendingOpt) {
						itemDesc = descendingOpt
					}
					if itemDesc { // Only include if true, as PTabler 'descending' defaults to false
						directive.descending = true
					}

					// Handle 'nulls_last'
					itemNullsLast := undefined // Let PTabler use its default if not specified
					if is_array(nullsLastOpt) {
						val := nullsLastOpt[i]
						ll.assert(is_bool(val), "Elements of 'nulls_last' array (at index %d) must be booleans. Got: %T", i, val)
						itemNullsLast = val
					} else if is_bool(nullsLastOpt) {
						itemNullsLast = nullsLastOpt
					}

					if !is_undefined(itemNullsLast) { // Only include if explicitly set
						directive.nullsLast = itemNullsLast
					}

					sortDirectives = append(sortDirectives, directive)
				}

				return {
					type: "sort",
					inputTable: dfName,
					outputTable: outputDfName,
					by: sortDirectives
				}
			})
			return _newDataFrame(parentWorkflow, outputDfName)
		},

		/**
		 * Joins this DataFrame with another DataFrame.
		 *
		 * @param rightDf {object} - The right DataFrame object to join with.
		 * @param opts {map} - A map of join options:
		 *   - `how` {string} (optional): Join strategy. One of 'inner', 'left', 'right', 'full', 'cross'. Defaults to 'inner'.
		 *   - `on` {string|array} (optional): Column name(s) to join on if they are the same in both DataFrames.
		 *                                      Cannot be used with `leftOn` or `rightOn`.
		 *   - `leftOn` {string|array} (optional): Column name(s) from the left DataFrame. Must be used with `rightOn`.
		 *   - `rightOn` {string|array} (optional): Column name(s) from the right DataFrame. Must be used with `leftOn`.
		 *   - `coalesce` {boolean} (optional): If true, identically named key columns are merged.
		 *                                       Does not apply to 'cross' join. PTabler default behavior applies if omitted.
		 *   - `leftColumns` {array} (optional): Array of maps `{column: "originalName", rename: "newName"}`
		 *                                      to select/rename columns from the left table.
		 *   - `rightColumns` {array} (optional): Array of maps `{column: "originalName", rename: "newName"}`
		 *                                       to select/rename columns from the right table.
		 * @returns {object} - A new DataFrame object representing the result of the join.
		 * @example
		 *   // Inner join on 'id' column
		 *   joinedDf := df1.join(df2, {on: "id"})
		 *
		 *   // Left join on different column names
		 *   joinedDf := df1.join(df2, {
		 *       how: "left",
		 *       leftOn: "user_id",
		 *       rightOn: "id",
		 *       rightColumns: [{column: "value", rename: "right_value"}]
		 *   })
		 *
		 *   // Cross join
		 *   crossJoinedDf := df1.join(df2, {how: "cross"})
		 */
		join: func(rightDf, opts) {
			if is_undefined(opts) || !is_map(opts) {
				ll.panic("join options must be a map argument.")
			}
			if is_undefined(rightDf) || is_undefined(rightDf._getDfName) || is_undefined(rightDf._getWorkflow) {
				ll.panic("rightDf argument for join must be a DataFrame object created by pt.workflow().frame().")
			}
			if rightDf._getWorkflow()._wfId != parentWorkflow._wfId {
				ll.panic("Both DataFrames in a join operation must belong to the same workflow.")
			}

			_ensureArrayLocal := func(val, argName) {
				if is_undefined(val) {
					return undefined
				}
				if is_string(val) {
					return [val]
				}
				if is_array(val) {
					for i, item in val {
						if !is_string(item) {
							ll.panic("'%s' option: if an array, all elements must be strings. Found type %T at index %d.", argName, item, i)
						}
					}
					return val
				}
				ll.panic("'%s' option must be a string or an array of strings. Got: %T", argName, val)
			}

			_validateAndSetColumnsLocal := func(columnsOpt, optName) {
				if is_undefined(columnsOpt) {
					return undefined
				}
				if !is_array(columnsOpt) {
					ll.panic("'%s' option must be an array of column mappings or strings. Got: %T", optName, columnsOpt)
				}
				mappings := []
				for i, item in columnsOpt {
					entry := undefined
					if is_string(item) {
						entry = { column: item }
					} else if is_map(item) {
						if is_undefined(item.column) || !is_string(item.column) {
							ll.panic("Each map mapping in '%s' at index %d must have a 'column' string field. Got: %v", optName, i, item.column)
						}
						entry = { column: item.column }
						if !is_undefined(item.rename) {
							if !is_string(item.rename) {
								ll.panic("The 'rename' field in '%s' mapping at index %d must be a string. Got: %T", optName, i, item.rename)
							}
							entry.rename = item.rename
						}
					} else {
						ll.panic("Each element in '%s' at index %d must be a string or a map. Got: %T", optName, i, item)
					}
					mappings = append(mappings, entry)
				}
				if len(mappings) == 0 { // PTabler schema might not like empty arrays for this.
					return undefined
				}
				return mappings
			}

			how := opts.how
			if is_undefined(how) {
				how = "inner" // Default join type
			}
			validHowTypes := ["inner", "left", "right", "full", "cross"]
			if !slices.hasElement(validHowTypes, how) {
				ll.panic("Invalid 'how' value for join: '%s'. Must be one of %v", how, validHowTypes)
			}

			onOpt := _ensureArrayLocal(opts.on, "on")
			leftOnOpt := _ensureArrayLocal(opts.leftOn, "leftOn")
			rightOnOpt := _ensureArrayLocal(opts.rightOn, "rightOn")

			outputDfName := parentWorkflow._newAnonymousDataFrameId()
			joinStep := {
				type: "join",
				leftTable: dfName,
				rightTable: rightDf._getDfName(),
				outputTable: outputDfName,
				how: how
			}

			if how == "cross" {
				if !is_undefined(onOpt) || !is_undefined(leftOnOpt) || !is_undefined(rightOnOpt) {
					ll.panic("'on', 'leftOn', 'rightOn' options cannot be used with 'cross' join.")
				}
				if !is_undefined(opts.coalesce) {
					ll.panic("'coalesce' option cannot be used with 'cross' join.")
				}
			} else { // 'inner', 'left', 'right', 'full'
				if !is_undefined(onOpt) {
					if !is_undefined(leftOnOpt) || !is_undefined(rightOnOpt) {
						ll.panic("For '%s' join, you must specify either 'on' option or both 'leftOn' and 'rightOn' options. 'on' was provided, but so was 'leftOn' or 'rightOn'.", how)
					}
					ll.assert(len(onOpt) > 0, "'on' option, if provided, cannot be an empty list.")
					joinStep.leftOn = onOpt
					joinStep.rightOn = onOpt
				} else if !is_undefined(leftOnOpt) && !is_undefined(rightOnOpt) {
					ll.assert(len(leftOnOpt) > 0, "'leftOn' option, if provided, cannot be an empty list.")
					ll.assert(len(rightOnOpt) > 0, "'rightOn' option, if provided, cannot be an empty list.")
					if len(leftOnOpt) != len(rightOnOpt) {
						ll.panic("'leftOn' and 'rightOn' lists must have the same number of columns. Got %d and %d.", len(leftOnOpt), len(rightOnOpt))
					}
					joinStep.leftOn = leftOnOpt
					joinStep.rightOn = rightOnOpt
				} else {
					ll.panic("For '%s' join, you must specify either 'on' option or both 'leftOn' and 'rightOn' options.", how)
				}

				if !is_undefined(opts.coalesce) {
					if !is_bool(opts.coalesce) {
						ll.panic("'coalesce' option must be a boolean. Got: %T", opts.coalesce)
					}
					joinStep.coalesce = opts.coalesce
				}
			}

			leftCols := _validateAndSetColumnsLocal(opts.leftColumns, "leftColumns")
			if !is_undefined(leftCols) {
				joinStep.leftColumns = leftCols
			}

			rightCols := _validateAndSetColumnsLocal(opts.rightColumns, "rightColumns")
			if !is_undefined(rightCols) {
				joinStep.rightColumns = rightCols
			}

			parentWorkflow.addRawStep(joinStep)
			return _newDataFrame(parentWorkflow, outputDfName)
		},

		/**
		 * Excludes a specific set of columns from the DataFrame.
		 *
		 * @param columns {array} - An array of column name strings to exclude.
		 * @returns {object} - A new DataFrame object with the specified columns removed.
		 * @example
		 *   dfWithoutCols := df.withoutColumns(["colA", "colB"])
		 */
		withoutColumns: func(...columns) {
			ll.assert(is_array(columns), "Argument to withoutColumns must be an array of column names.")
			ll.assert(len(columns) > 0, "Columns array for withoutColumns cannot be empty.")
			for i, colName in columns {
				ll.assert(is_string(colName), "Each element in the columns array for withoutColumns (at index %d) must be a string. Got: %T", i, colName)
			}

			outputDfName := parentWorkflow._newAnonymousDataFrameId()
			parentWorkflow.addRawStep({
				type: "without_columns",
				inputTable: dfName,
				outputTable: outputDfName,
				columns: columns
			})
			return _newDataFrame(parentWorkflow, outputDfName)
		}
	})
	return self
}

/**
 * Concatenates multiple DataFrames vertically.
 * Columns are matched by name. If the `columns` option is provided, only those columns
 * will be selected from all input DataFrames and included in the output.
 * All input DataFrames must belong to the same workflow.
 *
 * @param dataframes {array} - An array of DataFrame objects to concatenate.
 * @param ...optionsRaw {map} (optional) - A single map argument for additional options:
 *   - `columns` {array} (optional): An array of column name strings to select from all input DataFrames.
 *                                  If omitted, all columns are included. All input DataFrames must
 *                                  contain all specified columns if this option is used.
 * @returns {object} - A new DataFrame object representing the concatenated result.
 * @example
 *   // Assuming df1, df2, and df3 are DataFrames from the same workflow
 *   concatenatedDf := pt.concat([df1, df2, df3])
 *
 *   // Concatenate and select specific columns
 *   selectedColumnsDf := pt.concat(
 *       [df1, df2],
 *       {columns: ["id", "name", "value"]}
 *   )
 */
concat := func(dataframes, ...optionsRaw) {
	ll.assert(is_array(dataframes), "First argument to concat must be an array of DataFrame objects.")
	ll.assert(len(dataframes) > 0, "DataFrame array for concat cannot be empty.")

	opts := {}
	if len(optionsRaw) > 0 {
		if len(optionsRaw) == 1 && is_map(optionsRaw[0]) {
			opts = optionsRaw[0]
		} else {
			ll.panic("concat options must be a single map argument")
		}
	}

	parentWorkflow := undefined
	inputTableNames := []

	for i, df in dataframes {
		if is_undefined(df) || is_undefined(df._getDfName) || is_undefined(df._getWorkflow) {
			ll.panic("Argument at index %d in dataframes array is not a valid DataFrame object.", i)
		}
		currentWorkflow := df._getWorkflow()
		if is_undefined(parentWorkflow) {
			parentWorkflow = currentWorkflow
		} else if currentWorkflow._wfId != parentWorkflow._wfId {
			ll.panic("All DataFrames in a concat operation must belong to the same workflow.")
		}
		inputTableNames = append(inputTableNames, df._getDfName())
	}

	outputDfName := parentWorkflow._newAnonymousDataFrameId()

	concatenateStep := {
		type: "concatenate",
		inputTables: inputTableNames,
		outputTable: outputDfName
	}

	if !is_undefined(opts.columns) {
		if !is_array(opts.columns) {
			ll.panic("'columns' option for concat must be an array of strings.")
		}
		for i, colName in opts.columns {
			if !is_string(colName) {
				ll.panic("Each element in 'columns' option array (at index %d) must be a string. Got: %T", i, colName)
			}
		}
		concatenateStep.columns = opts.columns
	}

	parentWorkflow.addRawStep(concatenateStep)
	return _newDataFrame(parentWorkflow, outputDfName)
}

/**
 * Joins multiple DataFrames in a tree-like pattern using the specified join column.
 * The algorithm pairs DataFrames and joins them recursively until only one DataFrame remains.
 * This is particularly useful for joining many DataFrames on a common key where you want
 * to preserve all rows (full outer join with coalescing).
 *
 * Join tree structure example (for 6 DataFrames):
 * P0 ──┐
 *      ├─ J01 ──┐
 * P1 ──┘        │
 * P2 ──┐        ├─ J0123 ──┐
 *      ├─ J23 ──┘          │
 * P3 ──┘                   │
 * P4 ──┐                   ├─ J0123456 (Final Result)
 *      ├─ J45 ──┐          │
 * P5 ──┘        ├─ J456 ───┘
 * P6 ───────────┘
 *
 * @param dataframes {array} - An array of DataFrame objects to join in tree fashion.
 * @param opts {map} - Options map with required fields:
 *   - `on` {string}: The column name to join on (must exist in all DataFrames).
 * @returns {object} - A new DataFrame object representing the tree-joined result.
 * @example
 *   // Join multiple DataFrames on 'sample_id' column
 *   joinedDf := pt.treeJoin([df1, df2, df3, df4], {on: "sample_id"})
 *
 *   // With many DataFrames, this creates a balanced tree of joins
 *   result := pt.treeJoin(manyDataFrames, {on: "id"})
 */
treeJoin := func(dataframes, opts) {
	ll.assert(is_array(dataframes), "First argument to treeJoin must be an array of DataFrame objects.")
	ll.assert(len(dataframes) > 0, "DataFrame array for treeJoin cannot be empty.")

	if !is_map(opts) || is_undefined(opts.on) {
		ll.panic("treeJoin requires options with 'on' field")
	}
	if !is_string(opts.on) {
		ll.panic("'on' option must be a string")
	}

	// Single DataFrame case - just return it
	if len(dataframes) == 1 {
		return dataframes[0]
	}

	// Validate all DataFrames belong to the same workflow
	parentWorkflow := undefined
	for i, df in dataframes {
		if is_undefined(df) || is_undefined(df._getDfName) || is_undefined(df._getWorkflow) {
			ll.panic("Argument at index %d in dataframes array is not a valid DataFrame object.", i)
		}
		currentWorkflow := df._getWorkflow()
		if is_undefined(parentWorkflow) {
			parentWorkflow = currentWorkflow
		} else if currentWorkflow._wfId != parentWorkflow._wfId {
			ll.panic("All DataFrames in a treeJoin operation must belong to the same workflow.")
		}
	}

	pfs := dataframes

	// Tree joining algorithm: pair up DataFrames and join until only one remains
	for len(pfs) > 1 {
		nextPfs := []
		// Join consecutive pairs
		for i := 1; i < len(pfs); i += 2 {
			pf1 := pfs[i - 1]
			pf2 := pfs[i]
			joined := pf1.join(pf2, { on: opts.on, how: "full", coalesce: true })
			nextPfs = append(nextPfs, joined)
		}
		// If odd number of DataFrames, carry the last one to next iteration
		if len(pfs) % 2 == 1 {
			nextPfs = append(nextPfs, pfs[len(pfs) - 1])
		}
		pfs = nextPfs
	}

	ll.assert(len(pfs) == 1, "Expected exactly one data frame, got " + string(len(pfs)))
	return pfs[0]
}

export ll.toStrict({
	p: p,
	workflow: workflow,
	concat: concat,
	treeJoin: treeJoin,

	axis: exp.axis,
	col: exp.col,
	lit: exp.lit,
	concatStr: exp.concatStr,
	minHorizontal: exp.minHorizontal,
	maxHorizontal: exp.maxHorizontal,
	allHorizontal: exp.allHorizontal,
	anyHorizontal: exp.anyHorizontal,
	and: exp.and,
	or: exp.or,
	rank: exp.rank,
	when: exp.when,
	rawExp: exp.rawExp
})
