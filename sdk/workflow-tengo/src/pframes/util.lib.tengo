ll := import(":ll")
validation := import(":validation")
objects := import(":objects")
slices := import(":slices")
text := import("text")
smart := import(":smart")
constants := import(":pframes.constants")
pSpec := import(":pframes.spec")
path := import(":path")
json := import("json")
sets := import(":sets")
maps := import(":maps")

_PRE_PROCESS_STEP_SCHEMA := [`or`,
	{
		// Perform JS String.prototype.replace() operation
		// https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/String/replace
		`type`: `string,regex=regexpReplace`,
		// String representing ECMAScript RegEx with at least one capturing group.
		// If you need to reorder capturing groups - use RegExp matching the whole string
		// (must start with string begin anchor ^, end with string end anchor $).
		// Use regex playground https://regexr.com/ to test your ideas.
		`pattern`: `string`,
		// Replacement pattern used to construct result string from captured groups
		// https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/String/replace#specifying_a_string_as_the_replacement
		// Empty string as result would become NA.
		`replacement`: `string`
	},
	{
		// Simplified 'regexpReplace' with replacement set to $1.
		// This means that string is replaced with first capture group value.
		// Example 1:
		// - input: 123___abc.xlsx
		// - pattern: \d+___([a-z]+).xlsx
		// - result: abc
		// Example 2:
		// - input: 123___abc.xlsx
		// - pattern: (\d+)___([a-z]+).xlsx
		// - result: 123
		// Example 3:
		// - input: 123___abc.xlsx
		// - pattern: ((\d+)___([a-z]+)).xlsx
		// - result: 123___abc
		`type`: `string,regex=regexpExtract`,
		// String representing ECMAScript RegEx with at least one capturing group.
		// RegEx must match the entire string, this would be enforced even when ^ and $ are skipped.
		// If there are no matches - value would be replaced with empty string.
		// Wrong example:
		// - input: 123___abc.xlsx
		// - pattern: (\d+___[a-z]+)
		// - result: empty string, as .xlsx part is missing in pattern, so pattern was not matched
		// Correct example:
		// - input: 123___abc.xlsx
		// - pattern: (\d+___[a-z]+).xlsx
		// - result: 123___abc
		`pattern`: `string`
	}]

_SPEC_AXES_SCHEMA := {
	// Column label from XSV
	`column`: `string`,
	// Regular expression, if matched - whole row would be skipped;
	// Performed just after reading XSV before any other operations;
	// default: no filter
	`filterOutRegex,?`: `string`,
	// Pre-processing operations to be performed just after filtering out rows
	`preProcess,?`: [_PRE_PROCESS_STEP_SCHEMA],
	// Regular expression, if matched - value is considered to be N/A;
	// default: Int, Long - check that the value is a valid integer number;
	//          Float, Double - check that the value is a valid float number, not a NaN;
	//          String - accept anything;
	`naRegex,?`: `string`,
	// Should the N/A values (or empty rows) be allowed; When set to false,
	// an error would be thrown on N/A value (or empty row) encountering;
	// default: false
	`allowNA,?`: `bool`,
	// Specification of the axis
	`spec`: {
		// Type of axis values
		`type`: `string,regex=Int|Long|Float|Double|String`,
		// Name of the axis to be used in spec;
		// default: column label
		`name,?`: `string`,
		// Auxiliary information to the axis name, type and parents to form a unique identifier;
		// default: empty
		`domain,?`: { any: `string` },
		// Any additional information attached to the axis that does not affect its identifier;
		// default: "pl7.app/label": Column label from XSV
		`annotations,?`: { any: `string` },
		// A list of zero-based indices of parent axes in the overall axes specification;
		// default: empty
		`parentAxes,?`: [`number`]
	}
}

_SPEC_COLUMN_SCHEMA := {
	// Column label from XSV
	`column`: `string`,
	// Regular expression, if matched - whole row would be skipped;
	// Performed just after reading XSV before any other operations;
	// default: no filter
	`filterOutRegex,?`: `string`,
	// Pre-processing operations to be performed just after filtering out rows
	`preProcess,?`: [_PRE_PROCESS_STEP_SCHEMA],
	// Regular expression, if matched - value is considered to be N/A;
	// default: Int, Long - check that the value is a valid integer number;
	//          Float, Double - check that the value is a valid float number, not a NaN;
	//          String - accept anything;
	`naRegex,?`: `string`,
	// Should the N/A values (or empty rows) be allowed; When set to false,
	// an error would be thrown on N/A value (or empty row) encountering;
	// default: true
	`allowNA,?`: `bool`,
	// ID of the column to be used in spec and as a saved filename;
	// default: column label with all special characters replaced with '_'
	`id,?`: `string`,
	// Specification of the column
	`spec`: {
		// Type of column values
		`valueType`: `string,regex=Int|Long|Float|Double|String`,
		// Name of the column to be used in spec;
		// default: column label
		`name,?`: `string`,
		// Auxiliary information to the column name, type and parents to form a unique identifier;
		// default: empty
		`domain,?`: { any: `string` },
		// Any additional information attached to the axis that does not affect its identifier;
		// default: "pl7.app/label": Column label from XSV
		`annotations,?`: { any: `string` },
		// A list of zero-based indices of parent axes in the overall axes specification;
		// default: empty
		`parentAxes,?`: [ `number` ]
	}
}

_SPEC_INDEX_SCHEMA := {
	// Name of the axis
	`name`: `string`,
	// Auxiliary information to the column name, type and parents to form a unique identifier;
	// default: empty
	`domain,?`: { any: `string` },
	// Any additional information attached to the axis that does not affect its identifier;
	// default: "pl7.app/label": name
	`annotations,?`: { any: `string` },
	// A list of zero-based indices of parent axes in the overall axes specification;
	// default: empty
	`parentAxes,?`: [`number`]
}

PFCONV_IMPORT_CFG_SCHEMA := {
	// Single ASCII character to be used as separator;
	// default: ',' for .csv, '\t' for .tsv
	`separator,?`: `string`,
	// Single ASCII character, if XSV row begins with this character - the row would be skipped;
	// default: error is thrown if comment lines are found in document
	`commentLinePrefix,?`: `string`,
	// Should empty lines be skipped;
	// default: false
	`skipEmptyLines,?`: `bool`,
	// Resolve duplicates by adding sequential suffixes to column labels;
	// default: true
	`allowColumnLabelDuplicates,?`: `bool`,
	// XSV columns to use as PColumn axes, order of axes in resulting columns
	// would match the order of spec entries provided here
	`axes`: [_SPEC_AXES_SCHEMA],
	// Axis of type Long with XSV row numbers (would be the last one);
	// default: do not create additional axis with indexes
	`index,?`: _SPEC_INDEX_SCHEMA,
	// XSV columns to use as PColumn values, each exported individually
	`columns`: [_SPEC_COLUMN_SCHEMA],
	// When columns spec is provided but no such column was found in XSV - create such column
	// filled with NA values instead of failing;
	// default: false
	`allowArtificialColumns,?`: `bool`,
	// Prefix all column names with given string;
	// default: names would be preserved
	`columnNamePrefix,?`: `string`,
	// Columns would be stored using specified format;
	// default: Binary
	`storageFormat,?` : `string,regex=Binary|Json`,
	// Partitioning key length;
	// default: 0
	`partitionKeyLength,?` : `number`
}

_COLUMN_FILTER_SCHEMA := {
	// Match any of the types listed here;
	// default: type is ignored during matching
	`type,?`: `string,regex=Int|Long|Float|Double|String|Bytes`,
	// Match any of the names listed here;
	// default: name is ignored during matching
	`name,?`: `string`,
	// Match requires all the listed domains to have provided values;
	// default: no domain equality checks are performed during matching
	`domainValue,?`: { any: `string` },
	// Match requires all the listed annotations to have provided values;
	// default: no annotation equality checks are performed during matching
	`annotationValue,?`: { any: `string` },
	// Match requires all the listed annotations to match provided regex patterns;
	// default: no annotation pattern matching is performed during matching
	`annotationPattern,?`: { any: `string` }
}

// Validation of recursive schema requires forward declaration
_JOIN_ENTRY_SCHEMA := undefined
// Validation of recursive schema requires lazy evaluation
_JOIN_LEAF_SCHEMA := func(js, _, _) {
	return validation.checkJson(js, _JOIN_ENTRY_SCHEMA)
}
_JOIN_SUBTREE_SCHEMA := func(js, _, _) {
	return validation.checkJson(js, [_JOIN_ENTRY_SCHEMA])
}
// Validation of recursive schema instantiation
_JOIN_ENTRY_SCHEMA = [`or`,
    {
		// Node type discriminator
		`type`: `string,regex=column`,
		// Column id is a column specification filename without extension;
		// Example: column_id.spec, column_id.datainfo -> column_id
		`columnId`: `string`
	},
	{
		// Node type discriminator
		`type`: `string,regex=inner`,
		// Child nodes to be inner joined
		`entries`: _JOIN_SUBTREE_SCHEMA
	},
	{
		// Node type discriminator
		`type`: `string,regex=full`,
		// Child nodes to be full joined
		`entries`: _JOIN_SUBTREE_SCHEMA
	},
	{
		// Node type discriminator
		`type`: `string,regex=outer`,
		// Primes the join operation (left part of LEFT OUTER JOIN chain)
		`primary`: _JOIN_LEAF_SCHEMA,
		// Driven nodes, giving their values only if primary node have corresponding
		// nodes (right parts of LEFT OUTER JOIN chain)
		`secondary`: _JOIN_SUBTREE_SCHEMA
	}]

PFCONV_EXPORT_CFG_SCHEMA := {
	// String representing NA values of types Int, Long, Float, Double;
	// default: NaN
	`naStr,?`: `string`,
	// String representing NA and missing values of types String, Bytes;
	// default: null
	`nullStr,?`: `string`,
	// Toggle to export or skip axes and columns of type Bytes;
	// default: false
	`exportBytes,?`: `bool`,
	// Single ASCII character to be used as separator;
	// default: ',' for .csv, '\t' for .tsv
	`separator,?`: `string`,
	// Only columns matching the selector are exported;
	// default: all columns are exported
	`columnSelector,?`: [_COLUMN_FILTER_SCHEMA],
	// Explicitly specify core part of a join. The join would take there steps:
	// 1. From the PFrame specified columns are filtered using `columnSelector`;
	// 2. Core join is performed following the provided specification;
	// 3. Remaining selected columns are joined using the `joinType`;
	// default: all selected columns are joined using `joinType`
	`coreJoin,?`: _JOIN_ENTRY_SCHEMA,
	// During export, columns with equal axes specifications would be joined into single table
	// using the join type specified;
	// default: Full
	`joinType,?`: `string,regex=Full|Inner`
}

/**
 * Get column id from the column spec
 */
xsvColumnId := func(c) {
	id := c.id
	if is_undefined(id) {
		id = c.column
	}
	return id
}

/**
 * Removes all annotations & domain information from spec, to pass it to pfconv
 */
purifySpec := func(spec) {
	newSpec := copy(spec)

	newAxes := []
	for ax in spec.axes {
		newAxes = append(newAxes, objects.deleteUndefined({
			column: ax.column,
			filterOutRegex: ax.filterOutRegex,
			preProcess: ax.preProcess,
			naRegex: ax.naRegex,
			allowNA: ax.allowNA,
			spec: { type: ax.spec.type }
		}))
	}
	newSpec.axes = newAxes

	newCols := []
	for col in spec.columns {
		newCols = append(newCols, objects.deleteUndefined({
			column: col.column,
			filterOutRegex: col.filterOutRegex,
			preProcess: col.preProcess,
			naRegex: col.naRegex,
			allowNA: col.allowNA,
			id: col.id,
			spec: { valueType: col.spec.valueType }
		}))
	}
	newSpec.columns = newCols

	validation.assertType(newSpec, PFCONV_IMPORT_CFG_SCHEMA)

	return newSpec
}

/**
 * Split given pfconv config into:
 *
 *  (1) purified cfg with minimal required settings to be passed to the pfconv binary,
 *      to maximize caching retention when domains and annotations changes
 *  (2) complete columns specs, that can be directly rendered into BObjectSpecs and
 *      returned as workflow exports
 *
 * This method comes in handy in situations where actual csv -> pframe export is broadcasted
 * onto multiple objects, and then converges, so that each export operation only requires purified
 * config, and only data is saved for such operations, and actual specs are added somewhere
 * upstream.
 *
 * @param cfg  full pfconv config object
 * @param ops  additional options
 *               {
 *                 additionalAxesSpec  - array of additional axes spec to prepend to each column spec
 *               }
 *
 * @return {
 *           purifiedCfg  - purified pfconv config
 *           columnsSpec   - map of full column specs by ids from the original cfg
 *         }
 */
 // @TODO: rename to decomposeXsvImport to align with the naming in the rest xsv libs
decomposePfconvImportCfg := func(cfg, ...ops) {
	ll.assert(len(ops) <= 1, "too many options for decomposePfconvImportCfg: %v", len(ops))

	axesSpec := []
	if len(ops) == 1 && !is_undefined(ops[0].additionalAxesSpec) {
		axesSpec = ops[0].additionalAxesSpec
	}

	for axis in cfg.axes {
		axesSpec = append(axesSpec, axis.spec)
	}

	columnsSpec := {}

	for column in cfg.columns {
		id := xsvColumnId(column)
		ll.assert(is_undefined(columnsSpec[id]), "repeated column spec id in pfconv cfg: %v", id)
		columnsSpec[id] = column.spec
		columnsSpec[id].axesSpec = axesSpec
		// make sure spec contain correct kind value
		columnsSpec[id].kind = "PColumn"
	}

	return {
		purifiedCfg: purifySpec(cfg),
		columnsSpec: columnsSpec
	}
}

EMPTY_JSON_KEY := "[]" // string(json.encode([]))

/**
 * Calculates group indices from aggregation indices by removing aggregation indices from the full set of indices.
 * If no aggregation indices are provided, returns all indices.
 *
 * @param numberOfAxes: number - Total number of axes in the input
 * @param aggregationIndices: number[]|undefined - Indices to aggregate over. If undefined, returns all indices.
 *
 * @returns number[] - Array of remaining indices after removing aggregation indices, in ascending order.
 *                     If aggregationIndices is undefined, returns array [0..numberOfAxes-1]
 */
calculateGroupAxesIndices := func(numberOfAxes, aggregationIndices) {
	groupIndices := []
	for i:=0; i<numberOfAxes; i++ {
		groupIndices = append(groupIndices, i)
	}

	// groupIndices at this point is [0, 1, 2, ... numberOfAxes-1]

	if is_undefined(aggregationIndices) {
		return groupIndices
	}

	// copy and sort to prevent mutation of original array
	aggregationIndices = slices.quickSort(aggregationIndices)

	for j:=len(aggregationIndices)-1; j>=0; j-- {
		splice(groupIndices, aggregationIndices[j], 1)
	}
	return groupIndices
}

/**
 * Unpack p-frame resource into `{ [columnId]: { spec: json, data: resource } }` map
 */
pFrameToColumnsMap := func(pf) {
	columnsById := {}

	addColumn := func(key, value) {
		columnId := key[0:(len(key) - 5)]
		if is_undefined(columnsById[columnId]) {
			columnsById[columnId] = {}
		}

		if text.has_suffix(key, ".spec") {
			if smart.isResource(value) {
				value = value.getDataAsJson()
			}
			columnsById[columnId].spec = value
		} else if text.has_suffix(key, ".data") {
			columnsById[columnId].data = value
		} else {
			ll.panic("unknown suffix: %s", key)
		}
	}

	if smart.isResource(pf) {
		ll.assert(pf.checkResourceType(constants.RTYPE_P_FRAME), "Unexpected resource type: %v", pf.info().Type)
		for key, field in pf.inputs() {
			addColumn(key, field.getValue())
		}
	} else if ll.isMap(pf) {
		for key, value in pf {
			addColumn(key, value)
		}
	} else {
		ll.panic("unknown pf format: %v", pf)
	}

	return columnsById
}

/**
 * @param name: string - column name as to name it in the workdir
 * @param spec: json - column specification
 * @param data: resource - column data resource
 * @param exec: string - exec instance
 * @param ops: map - exec options
 *   - partitions: map - partitions filter
 *   - folder: string - folder to save the column in
 */
addColumnToWd := func(name, spec, data, execBuilder, ops) {
	ll.assert(is_string(name), "name must be a string, got: %v", name)

	ll.assert(!is_undefined(spec), "spec must be defined")
	validation.assertType(spec, pSpec.P_COLUMN_SPEC_SCHEMA)
	vt := spec.valueType
	ll.assert(vt == "Int" || vt == "Long" || vt == "Float" || vt == "Double" || vt == "String",
		"Unsupported value type: ", vt, spec)
	
    ll.assert(!is_undefined(data) && smart.isResource(data), "data must be a resource, got: %v", data)

	ll.assert(!is_undefined(execBuilder), "execBuilder must be defined")
	if !is_undefined(ops) {
		ll.assert(is_map(ops), "ops must be a map, got: %v", ops)
		validation.assertType(ops, {
			`__options__,closed`: true,
			`partitions,?`: { any: ["or", ["number"], ["string"]] },
			`folder,?`: "string"
		})
	}

	partitions := undefined
	if !is_undefined(ops.partitions) {
		partitions = {}
		for idx, keys in ops.partitions {
			partitions[idx] = sets.fromSlice(keys)
		}
	}

	acceptPartition := func(part) {
		if is_undefined(partitions) {
			return true
		}

		for idx, keys in partitions {
			if !sets.hasElement(keys, part[text.atoi(idx)]) {
				return false
			}
		}

		return true
	}

	makePath := func(fileName) {
		if is_undefined(ops.folder) {
			return fileName
		}
		return path.join(ops.folder, fileName)
	}

	if (data.checkResourceType(constants.RTYPE_P_COLUMN_DATA_JSON)) {

		ll.assert(is_undefined(partitions), "Partitions filter enabled while non-partitioned column exported")

		// write data info
		execBuilder.writeFile(makePath(name + ".datainfo"), json.encode({
			type: "JsonPartitioned",
			partitionKeyLength: 0,
			parts: { "[]": name + ".jdata" }
		}))

		// write jdata
		execBuilder.writeFile(makePath(name + ".jdata"), json.encode(data.getDataAsJson().data))

		// write spec
		execBuilder.writeFile(makePath(name + ".spec"), json.encode(spec))

	} else if (data.checkResourceType(constants.RTYPE_P_COLUMN_DATA_JSON_PARTITIONED)) {

        // write jdata
        i := 0
        parts := {}

		// using maps.getKeys to iterate over keys in the alphabetical order
		inputs := data.inputs()
        for k in maps.getKeys(inputs) {
			v := inputs[k]

			if !acceptPartition(json.decode(k)) {
				continue
			}

            part := name + i + ".jdata"
            parts[k] = part

            execBuilder.addFile(makePath(part), v.getValue())

            i = i + 1
        }

        // write data info
		execBuilder.writeFile(makePath(name + ".datainfo"), json.encode({
			type: "JsonPartitioned",
			partitionKeyLength: data.getDataAsJson().partitionKeyLength,
			parts: parts
		}))

        // write spec
		execBuilder.writeFile(makePath(name + ".spec"), json.encode(spec))

	} else if (data.checkResourceType(constants.RTYPE_P_COLUMN_DATA_JSON_SUPER_PARTITIONED)) {

		superPartitionKeyLength := data.getDataAsJson().superPartitionKeyLength
		partitionKeyLength := data.getDataAsJson().partitionKeyLength

		// write jdata
        i := 0
        parts := {}

		// iterate over super index
		for supKey, supData in data.inputs() {

			supKeyArray := json.decode(supKey)

			// iterate over inner index
			inputs := supData.getValue().inputs()
			for k in maps.getKeys(inputs) {
				v := inputs[k]

				part := name + i + ".jdata"

				innerKeyArray := json.decode(k)

				keyArray := supKeyArray + innerKeyArray

				if !acceptPartition(keyArray) {
					continue
				}

				key := json.encode(keyArray)

				parts[key] = part

            	execBuilder.addFile(makePath(part), v.getValue())

            	i = i + 1
			}
		}

		// write data info
		execBuilder.writeFile(makePath(name + ".datainfo"), json.encode({
			type: "JsonPartitioned",
			partitionKeyLength: superPartitionKeyLength + partitionKeyLength,
			parts: parts
		}))

        // write spec
		execBuilder.writeFile(makePath(name + ".spec"), json.encode(spec))

    } else if (data.checkResourceType(constants.RTYPE_P_COLUMN_DATA_BINARY_PARTITIONED)) {

		// write binary data
        i := 0
        parts := {}

        // using maps.getKeys to iterate over keys in the alphabetical order
		inputs := data.inputs()
        for k in maps.getKeys(inputs) {
			v := inputs[k]

            part := undefined

            if text.has_suffix(k, ".index") {
                part = k[0:(len(k) - 6)]

				if !acceptPartition(json.decode(part)) {
					continue
				}

                if is_undefined(parts[part]) {
                    parts[part] = {}
                }

                indexFile := name + i + ".index"

                execBuilder.addFile(makePath(indexFile), v.getValue())
                parts[part].index = indexFile
            } else if text.has_suffix(k, ".values") {
                part = k[0:(len(k) - 7)]

				if !acceptPartition(json.decode(part)) {
					continue
				}

                if is_undefined(parts[part]) {
                    parts[part] = {}
                }

                valuesFile := name + i + ".values"

                execBuilder.addFile(makePath(valuesFile), v.getValue())
                parts[part].values = valuesFile
            } else {
                ll.panic("unknown suffix: expected .index or .values, got ", k)
            }

			i = i + 1
        }

        // write data info
		execBuilder.writeFile(makePath(name + ".datainfo"), json.encode({
			type: "BinaryPartitioned",
			partitionKeyLength: data.getDataAsJson().partitionKeyLength,
			parts: parts
		}))

        // write spec
		execBuilder.writeFile(makePath(name + ".spec"), json.encode(spec))

	} else if (data.checkResourceType(constants.RTYPE_P_COLUMN_DATA_BINARY_SUPER_PARTITIONED)) {

		superPartitionKeyLength := data.getDataAsJson().superPartitionKeyLength
		partitionKeyLength := data.getDataAsJson().partitionKeyLength

		// write binary data
        i := 0
        parts := {}

		// iterate over super index
		for supKey, supData in data.inputs() {

			supKeyArray := json.decode(supKey)

			// iterate over inner index
			inputs := supData.getValue().inputs()
			for k in maps.getKeys(inputs) {
				v := inputs[k]

				if text.has_suffix(k, ".index") {
					innerKeyArray := json.decode(k[0:(len(k) - 6)])
					keyArray := supKeyArray + innerKeyArray

					if !acceptPartition(keyArray) {
						continue
					}

					key := json.encode(keyArray)

					if is_undefined(parts[key]) {
						parts[key] = {}
					}

					indexFile := name + i + ".index"

					execBuilder.addFile(makePath(indexFile), v.getValue())
					parts[key].index = indexFile
				} else if text.has_suffix(k, ".values") {

					innerKeyArray := json.decode(k[0:(len(k) - 7)])

					keyArray := supKeyArray + innerKeyArray

					if !acceptPartition(keyArray) {
						continue
					}

					key := json.encode(keyArray)

					if is_undefined(parts[key]) {
						parts[key] = {}
					}

					valuesFile := name + i + ".values"

					execBuilder.addFile(makePath(valuesFile), v.getValue())
					parts[key].values = valuesFile
				} else {
					ll.panic("unknown suffix: expected .index or .values, got ", k)
				}

				i = i + 1
			}
		}

		// write data info
		execBuilder.writeFile(makePath(name + ".datainfo"), json.encode({
			type: "BinaryPartitioned",
			partitionKeyLength: superPartitionKeyLength + partitionKeyLength,
			parts: parts
		}))

        // write spec
		execBuilder.writeFile(makePath(name + ".spec"), json.encode(spec))

	} else if (data.checkResourceType(constants.RTYPE_P_COLUMN_DATA_PARQUET_PARTITIONED)) {

		// write parquet data
        i := 0
        parts := {}

		// using maps.getKeys to iterate over keys in the alphabetical order
		inputs := data.inputs()
        for k in maps.getKeys(inputs) {
			v := inputs[k]

			if !acceptPartition(json.decode(k)) {
				continue
			}

			parquetChunkResource := v.getValue()
			parquetChunkMetadata := parquetChunkResource.getDataAsJson()
			parquetChunkMapping := parquetChunkResource.getMetaInputs()["mapping"].getDataAsJson()
			parquetChunkDataResource := parquetChunkResource.getMetaInputs()["blob"]

            parquetChunkDataPath := "__resource__" + parquetChunkDataResource.id + ".parquet"
			parquetChunkDataFile := parquetChunkDataResource.getValue()

            parts[k] = maps.merge({ data: parquetChunkDataPath }, parquetChunkMetadata, parquetChunkMapping)

            execBuilder.addFile(makePath(parquetChunkDataPath), parquetChunkDataFile)

            i = i + 1
        }

        // write data info
		execBuilder.writeFile(makePath(name + ".datainfo"), json.encode({
			type: "ParquetPartitioned",
			partitionKeyLength: data.getDataAsJson().partitionKeyLength,
			parts: parts
		}))

        // write spec
		execBuilder.writeFile(makePath(name + ".spec"), json.encode(spec))

	} else if (data.checkResourceType(constants.RTYPE_P_COLUMN_DATA_PARQUET_SUPER_PARTITIONED)) {

		superPartitionKeyLength := data.getDataAsJson().superPartitionKeyLength
		partitionKeyLength := data.getDataAsJson().partitionKeyLength

		// write jdata
        i := 0
        parts := {}

		// iterate over super index
		for supKey, supData in data.inputs() {

			supKeyArray := json.decode(supKey)

			// iterate over inner index
			inputs := supData.getValue().inputs()
			for k in maps.getKeys(inputs) {
				v := inputs[k]

				innerKeyArray := json.decode(k)

				keyArray := supKeyArray + innerKeyArray

				if !acceptPartition(keyArray) {
					continue
				}

				key := json.encode(keyArray)

				parquetChunkResource := v.getValue()
				parquetChunkMetadata := parquetChunkResource.getDataAsJson()
				parquetChunkMapping := parquetChunkResource.getMetaInputs()["mapping"].getDataAsJson()
				parquetChunkDataResource := parquetChunkResource.getMetaInputs()["blob"]

				parquetChunkDataPath := "__resource__" + parquetChunkDataResource.id + ".parquet"
				parquetChunkDataFile := parquetChunkDataResource.getValue()

				parts[key] = maps.merge({ data: parquetChunkDataPath }, parquetChunkMetadata, parquetChunkMapping)

				execBuilder.addFile(makePath(parquetChunkDataPath), parquetChunkDataFile)

            	i = i + 1
			}
		}

		// write data info
		execBuilder.writeFile(makePath(name + ".datainfo"), json.encode({
			type: "ParquetPartitioned",
			partitionKeyLength: superPartitionKeyLength + partitionKeyLength,
			parts: parts
		}))

        // write spec
		execBuilder.writeFile(makePath(name + ".spec"), json.encode(spec))

	} else {
		ll.panic("unsupported p-column data type", data.info().Type)
	}
}

export ll.toStrict({
	PFCONV_IMPORT_CFG_SCHEMA: PFCONV_IMPORT_CFG_SCHEMA,
	PFCONV_EXPORT_CFG_SCHEMA: PFCONV_EXPORT_CFG_SCHEMA,
	EMPTY_JSON_KEY: EMPTY_JSON_KEY,

	xsvColumnId: xsvColumnId,
	purifySpec: purifySpec,
	decomposePfconvImportCfg: decomposePfconvImportCfg,
	calculateGroupAxesIndices: calculateGroupAxesIndices,
	pFrameToColumnsMap: pFrameToColumnsMap,
	addColumnToWd: addColumnToWd
})
