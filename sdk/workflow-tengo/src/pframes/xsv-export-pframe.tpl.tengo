//
// export p-frame to csv
//

validation := import(":validation")
exec := import(":exec")
json := import("json")
text := import("text")
util := import(":pframes.util")
assets := import(":assets")
constants := import(":pframes.constants")
ll := import(":ll")

self := import(":tpl.light")

pfconvSw := assets.importSoftware("@milaboratories/software-pframes-conv:main")

self.defineOutputs(["result"])

self.body(func(inputs) {
    pf := inputs.pf
    xsvType := inputs.xsvType
    params := inputs.params
	ops := inputs.ops

	validation.assertType(params, util.PFCONV_EXPORT_CFG_SCHEMA)

    resultFile := "result." + xsvType
	frameFolder := "frame"

	// convert p-frame to csv
	pfconv := exec.builder().
		software(pfconvSw).
		printErrStreamToStdout().
		arg("exportCsv").
		arg("-p").arg("params.json").
		arg("-o").arg(resultFile).
        arg(frameFolder).
        writeFile("params.json", json.encode(params)).
		saveFile(resultFile)

	if !is_undefined(ops) {
		if !is_undefined(ops.queue) {
			pfconv.setQueue(ops.queue)
		} else {
			pfconv.inLightQueue()
		}

		if !is_undefined(ops.cpu) {
			pfconv.cpu(ops.cpu)
		}

		if !is_undefined(ops.mem) {
			pfconv.mem(ops.mem)
		}

		if !is_undefined(ops.inputCache) {
			pfconv.cacheInputs(ops.inputCache)
		}
	} else {
		pfconv.inLightQueue()
	}

	for name, ds in util.pFrameToColumnsMap(pf) {
		ll.assert(ds.data.checkResourceType(constants.RTYPE_P_COLUMN_DATA_JSON) ||
			ds.data.checkResourceType(constants.RTYPE_P_COLUMN_DATA_JSON_PARTITIONED) ||
			ds.data.checkResourceType(constants.RTYPE_P_COLUMN_DATA_JSON_SUPER_PARTITIONED) ||
			ds.data.checkResourceType(constants.RTYPE_P_COLUMN_DATA_BINARY_PARTITIONED) ||
			ds.data.checkResourceType(constants.RTYPE_P_COLUMN_DATA_BINARY_SUPER_PARTITIONED),
			"p-column type %v is not supported by pfconv", ds.data.info().Type)
		util.addColumnToWd(name, ds.spec, ds.data, pfconv, {
			partitions: ops.partitions,
			folder: frameFolder
		})
	}

	r := pfconv.run()

	return {
        result: r.getFile(resultFile)
    }
})
