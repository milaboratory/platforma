//
// export p-frame to csv
//

exec := import(":exec")
json := import("json")
text := import("text")
util := import(":pframes.util")
assets := import(":assets")
constants := import(":pframes.constants")
ll := import(":ll")

self := import(":tpl.light")

pfconvSw := assets.importSoftware("@milaboratories/software-pframes-conv:main")

self.defineOutputs(["result"])

self.body(func(inputs) {
    pf := inputs.pf
    xsvType := inputs.xsvType
    params := inputs.params

	queue := inputs.queue
	cpu := inputs.cpu
	mem := inputs.mem
	inputCache := inputs.inputCache

    resultFile := "result." + xsvType
	frameFolder := "frame"

	// convert p-frame to csv
	pfconv := exec.builder().
		software(pfconvSw).
		printErrStreamToStdout().
		arg("exportCsv").
		arg("-p").arg("params.json").
		arg("-o").arg(resultFile).
        arg(frameFolder).
        writeFile("params.json", json.encode(params)).
		saveFile(resultFile)

	pfconv.setQueue(queue)
	if !is_undefined(cpu) {
		pfconv.cpu(cpu)
	}
	if !is_undefined(mem) {
		pfconv.mem(mem)
	}
	if !is_undefined(inputCache) {
		pfconv.cacheInputs(inputCache)
	}

	for name, ds in util.pFrameToColumnsMap(pf) {
		ll.assert(ds.data.checkResourceType(constants.RTYPE_P_COLUMN_DATA_JSON) ||
			ds.data.checkResourceType(constants.RTYPE_P_COLUMN_DATA_JSON_PARTITIONED) ||
			ds.data.checkResourceType(constants.RTYPE_P_COLUMN_DATA_JSON_SUPER_PARTITIONED) ||
			ds.data.checkResourceType(constants.RTYPE_P_COLUMN_DATA_BINARY_PARTITIONED) ||
			ds.data.checkResourceType(constants.RTYPE_P_COLUMN_DATA_BINARY_SUPER_PARTITIONED),
			"p-column type %v is not supported by pfconv", ds.data.info().Type)
		util.addColumnToWd(name, ds.spec, ds.data, pfconv, {
			folder: frameFolder
		})
	}

	r := pfconv.run()

	return {
        result: r.getFile(resultFile)
    }
})
