// pframes ll aggregate

self := import(":tpl.light")

ll := import(":ll")
render := import(":render")
smart := import(":smart")
pConstants := import(":pframes.constants")
pUtil := import(":pframes.util")
xsv := import(":pframes.xsv")
builder := import(":pframes.builder")
removeNullsLib := import(":pframes.remove-nulls")
text := import("text")

json := import("json")

self.awaitState("InputsLocked")
self.awaitState("data", "InputsLocked")
self.awaitState("params", "ResourceReady")

self.body(func(inputs) {

	// parameters of aggregation
	//   indices: int[]
	//   eph: boolean
	//   passKey: boolean
	//   outputs: (string)[]
	//   expectedKeyLength: boolean
	params := inputs.params

	if is_undefined(params) {
		ll.panic("Params is not defined")
	}

	if !is_map(params) {
		ll.panic("Params have wrong type, or nor ready yet: %v", params)
	}

	aggregationIndices := params.aggregationIndices
	eph := params.eph
	outputs := params.outputs
	passKey := params.passKey
	expectedKeyLength := params.expectedKeyLength

	if !is_array(aggregationIndices) && !is_undefined(aggregationIndices) {
		ll.panic("Wrong indices parameter type: %v", aggregationIndices)
	}

	if !is_array(outputs) || len(outputs) == 0 {
		ll.panic("Absent, empty or malformed outputs list: %v", params)
	}

	// map of resources to aggregate over
	data := inputs.data

	// template implementing aggregation body
	body := inputs.body

	if is_undefined(data) {
		ll.panic("Data is not defined")
	}

	if is_undefined(body) {
		ll.panic("Body is not defined")
	}

	extraInputs := {}
	for name, field in inputs {
		if name != "params" && name != "data" && name != "body" {
			if name[:8] != "__extra_" {
				ll.panic("Unexpected input: %v", name)
			}
			extraInputs[name[8:]] = field
		}
	}

	// Check supported types
	isResourceMap := data.checkResourceType(pConstants.RTYPE_P_COLUMN_DATA_RESOURCE_MAP)
	isJsonPartitioned := data.checkResourceType(pConstants.RTYPE_P_COLUMN_DATA_JSON_PARTITIONED)
	isBinaryPartitioned := data.checkResourceType(pConstants.RTYPE_P_COLUMN_DATA_BINARY_PARTITIONED)
	isPartitioned := isJsonPartitioned || isBinaryPartitioned

	if isResourceMap || isPartitioned {
		meta := data.getDataAsJson()

		inputKeyLength := 0
		if isResourceMap {
			if !is_map(meta) || is_undefined(meta.keyLength) {
				 ll.panic("Invalid metadata for ResourceMap: %v", meta)
			}
			inputKeyLength = meta.keyLength
		} else { // isPartitioned
			if !is_map(meta) || is_undefined(meta.partitionKeyLength) {
				 ll.panic("Invalid metadata for Partitioned resource: %v", meta)
			}
			inputKeyLength = meta.partitionKeyLength
		}


		if !is_undefined(expectedKeyLength) {
			// Check against the relevant key length (full key for ResourceMap, partition key for Partitioned)
			if expectedKeyLength != inputKeyLength {
				ll.panic("Unexpected key length: %v != %v", expectedKeyLength, inputKeyLength)
			}
		}

		// group key -> group resource
		groups := {}

		// mapping or aggregation mode
		mappingMode := is_undefined(aggregationIndices)

		// Partitioned inputs cannot be used in mapping mode as the body expects individual elements, not partitions.
		if mappingMode && isPartitioned {
			ll.panic("Partitioned PColumn data types (JsonPartitioned, BinaryPartitioned) cannot be used in mapping mode (aggregationIndices not set). Use aggregation mode instead.")
		}

		groupKeyLength := 0

		if mappingMode {
			groupKeyLength = inputKeyLength
			for sKey, field in data.inputs() {
				groups[sKey] = field
			}
		} else { // <- aggregation mode
			// Calculate group indices based on inputKeyLength (full key for ResourceMap, partition key for Partitioned)
			groupIndices := pUtil.calculateGroupAxesIndices(inputKeyLength, aggregationIndices)
			groupKeyLength = len(groupIndices)
			groupElementKeyLength := inputKeyLength - len(groupIndices)
			if groupElementKeyLength < 0 {
				ll.panic("Group element key length is negative: %v", groupElementKeyLength)
			}
			if !isPartitioned {
				ll.assert(groupElementKeyLength == len(aggregationIndices),
				  "Group element key length is not equal to aggregation indices length: %v != %v",
				  groupElementKeyLength, len(aggregationIndices))
			}

			for sKey, field in data.inputs() {
				sKeyP := sKey
				suffix := ""
				if isBinaryPartitioned {
					dotIdx := text.last_index(sKey, ".")
					if dotIdx == -1 {
						ll.panic("BinaryPartitioned key must contain a dot: %v", sKey)
					}
					suffix = sKey[dotIdx:]
					sKeyP = sKey[:(dotIdx)]
				}
				key := json.decode(sKeyP) // Key is the resource key (ResourceMap) or partition key (Partitioned)

				groupKey := []
				for i in groupIndices {
					if len(key) <= i {
						ll.panic("Key length is less than aggregation index: %v, key: %v", i, key)
					}
					groupKey = append(groupKey, key[i])
				}

				inGroupKey := []
				for i in aggregationIndices {
					if len(key) <= i {
						if isJsonPartitioned || isBinaryPartitioned {
							continue
						} else {
							ll.panic("Key length is less than aggregation index: %v, key: %v", i, key)
						}
					}
					inGroupKey = append(inGroupKey, key[i])
				}

				sGroupKey := json.encode(groupKey)
				// This is the key for the element within the intermediate group ResourceMap
				sInGroupKey := json.encode(inGroupKey)

				group := groups[sGroupKey]
				if is_undefined(group) {
					// Create an intermediate resource to hold aggregated elements.
					// Its type must match the input data type as expected by the body template.
					intermediateGroupType := undefined
					intermediateGroupMetadata := undefined

					if isBinaryPartitioned {
						intermediateGroupType = pConstants.RTYPE_P_COLUMN_DATA_BINARY_PARTITIONED
						intermediateGroupMetadata = { partitionKeyLength: groupElementKeyLength }
					} else if isJsonPartitioned {
						intermediateGroupType = pConstants.RTYPE_P_COLUMN_DATA_JSON_PARTITIONED
						intermediateGroupMetadata = { partitionKeyLength: groupElementKeyLength }
					} else { // isResourceMap (this is the only remaining option)
						intermediateGroupType = pConstants.RTYPE_P_COLUMN_DATA_RESOURCE_MAP
						intermediateGroupMetadata = { keyLength: groupElementKeyLength }
					}

					group = smart.structBuilder(
						intermediateGroupType,
						json.encode(intermediateGroupMetadata)
					)
					groups[sGroupKey] = group
				}
				group.createInputField(string(sInGroupKey) + suffix).set(field)
			}
		}

		outputMaps := {}
		optionalOutputs := {}

		for output in outputs {
			if !is_map(output) {
				ll.panic("malformed output: %v", output)
			}

			reultMightBeNull := false
			for p in output.path {
				if is_map(p) && p.optional {
					reultMightBeNull = true
					break
				}
			}
			if reultMightBeNull {
				optionalOutputs[output.name] = true
			}

			if output.type == "Resource" {
				outputMaps[output.name] = smart.structBuilder(
					pConstants.RTYPE_P_COLUMN_DATA_RESOURCE_MAP,
					json.encode({ keyLength: groupKeyLength })
				)
			} else if output.type == "ResourceMap" {
				outputMaps[output.name] = smart.structBuilder(
					pConstants.RTYPE_P_COLUMN_DATA_RESOURCE_MAP_PARTITIONED,
					json.encode({
						partitionKeyLength: groupKeyLength,
						keyLength: output.keyLength
					})
				)
			} else if output.type == "JsonPartitioned" {
				outputMaps[output.name] = smart.structBuilder(
					pConstants.RTYPE_P_COLUMN_DATA_JSON_SUPER_PARTITIONED,
					json.encode({
						superPartitionKeyLength: groupKeyLength,
						partitionKeyLength: output.partitionKeyLength
					})
				)
			} else if output.type == "BinaryPartitioned" {
				outputMaps[output.name] = smart.structBuilder(
					pConstants.RTYPE_P_COLUMN_DATA_BINARY_SUPER_PARTITIONED,
					json.encode({
						superPartitionKeyLength: groupKeyLength,
						partitionKeyLength: output.partitionKeyLength
					})
				)
			} else if output.type == "Xsv" {
				// each field will contain super-partitioned data for the corresponding column
				// (specs must be added on the higher level to make a pframe)
				map := {}
				for column in output.settings.columns {
					id := pUtil.xsvColumnId(column)
					map[id] = {} // partKey -> partitioned data resource ref (not-super-partitioned)
				}
				// for regular outputs:
				//   [outputName][groupKey]
				// for xsv output:
				//   [outputName][columnId][groupKey]
				outputMaps[output.name] = map
			} else {
				ll.panic("unknonw output type: %v", output.type)
			}
		}

		for sGroupKey, groupValue in groups {
			renderInputs := {}

			if passKey {
				renderInputs[pConstants.KEY_FIELD_NAME] = json.decode(sGroupKey)
			}

			if mappingMode {
				// groupValue here is just a field ref to input map element
				renderInputs[pConstants.VALUE_FIELD_NAME] = groupValue
			} else {
				// in this case groupValue is a resource constructed above
				renderInputs[pConstants.VALUE_FIELD_NAME] = groupValue.lockAndBuild()
			}

			for name, field in extraInputs {
				renderInputs[name] = field
			}

			renderResult := render.createUniversal(body, eph, renderInputs)
			for output in outputs {
				path := output.path
				if is_undefined(path) {
					path = [output.name]
				}

				ref := renderResult.resolveOutput(path)
				reultMightBeNull := optionalOutputs[output.name]

				if output.type == "Xsv" {
					pf := xsv.importFile(ref, output.xsvType, output.settings, { dataOnly: true, allowNullInput: reultMightBeNull })
					for col in output.settings.columns {
						id := pUtil.xsvColumnId(col)
						data := pf.getFutureInputField({name: id, optional: reultMightBeNull})
						outputMaps[output.name][id][sGroupKey] = data
					}
				} else {
					outputMaps[output.name].
						createInputField(sGroupKey).
						set(ref)
				}
			}
		}

		for output in outputs {
			resultMightBeNull := optionalOutputs[output.name]

			if output.type == "Xsv" {

				partitionKeyLength := 0
				if !is_undefined(output.settings.partitionKeyLength) {
					partitionKeyLength = output.settings.partitionKeyLength
				}

				binary := false // json or binary
				if output.settings.storageFormat == "Binary" {
					binary = true
				} else if output.settings.storageFormat != "Json" {
					ll.panic("Unknow storage format: %v", output.settings.storageFormat)
				}

				// if nested xsv import is known to produce unpartitioned output we don't create
				// redundant super-partitioned structures
				unwrapZeroPartition := partitionKeyLength == 0
				// if groupKeyLength == 0 we will directly return the result of xsv.importFile
				// without adding trivial super-partitioning layer
				unwrapZeroGroupKey := groupKeyLength == 0

				dataResType := undefined

				if binary {
					if unwrapZeroPartition {
						dataResType = pConstants.RTYPE_P_COLUMN_DATA_BINARY_PARTITIONED
					} else {
						dataResType = pConstants.RTYPE_P_COLUMN_DATA_BINARY_SUPER_PARTITIONED
					}
				} else {
					if unwrapZeroPartition {
						dataResType = pConstants.RTYPE_P_COLUMN_DATA_JSON_PARTITIONED
					} else {
						dataResType = pConstants.RTYPE_P_COLUMN_DATA_JSON_SUPER_PARTITIONED
					}
				}

				dataResData := undefined
				if unwrapZeroPartition {
					dataResData = json.encode({
						partitionKeyLength: groupKeyLength
					})
				} else {
					dataResData = json.encode({
						superPartitionKeyLength: groupKeyLength,
						partitionKeyLength: partitionKeyLength
					})
				}

				// aggragation column results from all iterations into super-partitioned structures
				// or unwrap them and pass as is if groupKeyLength == 0

				columnResults := {}
				for col in output.settings.columns {
					id := pUtil.xsvColumnId(col)
					parts := outputMaps[output.name][id]
					if unwrapZeroGroupKey {
						ll.assert(len(parts) == 1, "Expected exactly one partition for unwrapping zero group key")
						data := parts[pUtil.EMPTY_JSON_KEY]
						ll.assert(!is_undefined(data), "Expected part \"[]\" to be defined for unwrapping zero group key")
						columnResults[id] = data
					} else {
						dataRes := smart.structBuilder(dataResType, dataResData)
						for sGroupKey, partData in parts {
							if unwrapZeroPartition {
								if binary {
									for suffix in [".index", ".values"] {
										dataRes.createInputField(sGroupKey + suffix).set(partData.buildFutureField({name: pUtil.EMPTY_JSON_KEY + suffix, optional: true}))
									}
								} else {
									dataRes.createInputField(sGroupKey).set(partData.buildFutureField({name: pUtil.EMPTY_JSON_KEY, optional: true}))
								}
							} else {
								dataRes.createInputField(sGroupKey).set(partData)
							}
						}
						columnResults[id] = removeNullsLib.removeNulls(dataRes.lockAndBuild())
					}
				}

				if !is_undefined(output.flattenWithDelimiter) {
					for id, data in columnResults {
						outputMaps[output.name + output.flattenWithDelimiter + id] = data
					}
				} else {
					mapBuilder := builder.pFrameBuilder()
					for id, data in columnResults {
						mapBuilder.add(id, undefined, data)
					}
					outputMaps[output.name] = mapBuilder.build()
				}
			} else {
				if resultMightBeNull {
					outputMaps[output.name] = removeNullsLib.removeNulls(outputMaps[output.name].lockAndBuild())
				} else {
					outputMaps[output.name] = outputMaps[output.name].lockAndBuild()
				}
			}
		}

		return outputMaps
	} else {
		ll.panic("Unexpected input resource type: %v", data.info().Type)
	}
})
