// pframes ll aggregate

self := import(":tpl.light")

ll := import(":ll")
render := import(":render")
smart := import(":smart")
pConstants := import(":pframes.constants")
pUtil := import(":pframes.util")
xsv := import(":pframes.xsv")
builder := import(":pframes.builder")
removeNullsLib := import(":pframes.remove-nulls")
text := import("text")
anonymize := import(":anonymize")
assets := import(":assets")

json := import("json")

parseToJsonTpl := assets.importTemplate(":pframes.parse-to-json")

// Helper function to extract column ID with backward compatibility
getColumnId := func(column) {
	kind := column.kind
	if is_undefined(kind) {
		kind = "column"
	}

	if kind == "column" {
		columnId := column.id
		if is_undefined(columnId) {
			columnId = column.column
		}
		return columnId
	} else if kind == "line" || kind == "json-line" {
		return column.id
	} else {
		ll.panic("Unknown column kind: %s", kind)
	}
}

self.awaitState("InputsLocked")
self.awaitState("data", "InputsLocked")
self.awaitState("params", "ResourceReady")

self.body(func(inputs) {

	// parameters of aggregation
	//   indices: int[]
	//   eph: boolean
	//   passKey: boolean
	//   outputs: (string)[]
	//   expectedKeyLength: int
	//   stepCache?: int
	params := inputs.params

	if is_undefined(params) {
		ll.panic("Params is not defined")
	}

	if !is_map(params) {
		ll.panic("Params have wrong type, or nor ready yet: %v", params)
	}

	aggregationIndices := params.aggregationIndices
	eph := params.eph
	outputs := params.outputs
	passKey := params.passKey
	expectedKeyLength := params.expectedKeyLength
	anonymizationIndices := params.anonymizationIndices

	stepCache := params.stepCache
	applyCache := func(field) {
		if !is_undefined(stepCache) && stepCache > 0 {
			field.setCache(stepCache)
		}
		return field
	}

	if !is_array(aggregationIndices) && !is_undefined(aggregationIndices) {
		ll.panic("Wrong indices parameter type: %v", aggregationIndices)
	}

	if !is_array(outputs) || len(outputs) == 0 {
		ll.panic("Absent, empty or malformed outputs list: %v", params)
	}

	// map of resources to aggregate over
	data := inputs.data

	// template implementing aggregation body
	body := inputs.body

	if is_undefined(data) {
		ll.panic("Data is not defined")
	}

	if is_undefined(body) {
		ll.panic("Body is not defined")
	}

	extraInputs := {}
	metaInputs := {}
	for name, field in self.rawInputs() {
		if name != "params" && name != "data" && name != "body" {
			if name[:8] == "__extra_" {
				extraInputs[name[8:]] = field
			} else if name[:7] == "__meta_" {
				metaInputs[name[7:]] = field
			} else {
				ll.panic("Unexpected input: %v", name)
			}
		}
	}

	// Check for unsupported combinations
	if !is_undefined(anonymizationIndices) && len(anonymizationIndices) > 0 {
		// Check if input is PColumnData/Json which doesn't support anonymization
		isJson := data.checkResourceType(pConstants.RTYPE_P_COLUMN_DATA_JSON)
		if isJson {
			ll.panic("Anonymization is not supported for PColumnData/Json. Anonymization currently works only with resources.")
		}
	}

	// Check supported types
	isResourceMap := data.checkResourceType(pConstants.RTYPE_P_COLUMN_DATA_RESOURCE_MAP)
	isJsonPartitioned := data.checkResourceType(pConstants.RTYPE_P_COLUMN_DATA_JSON_PARTITIONED)
	isBinaryPartitioned := data.checkResourceType(pConstants.RTYPE_P_COLUMN_DATA_BINARY_PARTITIONED)
	isParquetPartitioned := data.checkResourceType(pConstants.RTYPE_P_COLUMN_DATA_PARQUET_PARTITIONED)
	isJson := data.checkResourceType(pConstants.RTYPE_P_COLUMN_DATA_JSON)
	isPartitioned := isJsonPartitioned || isBinaryPartitioned || isParquetPartitioned

	if isResourceMap || isPartitioned || isJson {
		meta := data.getDataAsJson()

		inputKeyLength := 0
		if isResourceMap {
			if !is_map(meta) || is_undefined(meta.keyLength) {
				 ll.panic("Invalid metadata for ResourceMap: %v", meta)
			}
			inputKeyLength = meta.keyLength
		} else if isPartitioned {
			if !is_map(meta) || is_undefined(meta.partitionKeyLength) {
				 ll.panic("Invalid metadata for Partitioned resource: %v", meta)
			}
			inputKeyLength = meta.partitionKeyLength
		} else if isJson {
			if !is_map(meta) || is_undefined(meta.keyLength) || is_undefined(meta.data) {
				 ll.panic("Invalid metadata for Json: %v", meta)
			}
			inputKeyLength = meta.keyLength
		}


		if !is_undefined(expectedKeyLength) {
			// Check against the relevant key length (full key for ResourceMap, partition key for Partitioned)
			if expectedKeyLength != inputKeyLength {
				ll.panic("Unexpected key length: %v != %v", expectedKeyLength, inputKeyLength)
			}
		}

		// group key -> group resource
		groups := {}

		// mapping or aggregation mode
		mappingMode := is_undefined(aggregationIndices)

		// Partitioned inputs cannot be used in mapping mode as the body expects individual elements, not partitions.
		if mappingMode && isPartitioned {
			ll.panic("Partitioned PColumn data types (JsonPartitioned, BinaryPartitioned) cannot be used in mapping mode (aggregationIndices not set). Use aggregation mode instead.")
		}

		groupKeyLength := 0

		if mappingMode {
			groupKeyLength = inputKeyLength

			if isJson {
				// For PColumnData/Json in mapping mode, iterate through embedded data
				embeddedData := meta.data
				for sKey, value in embeddedData {
					// Create a simple JSON resource containing the primitive value
					valueResource := smart.createJsonResource(value)
					groups[sKey] = valueResource
				}
			} else {
				// Existing logic for ResourceMap
				for sKey, field in data.inputs() {
					groups[sKey] = field
				}
			}
		} else { // <- aggregation mode
			// Calculate group indices based on inputKeyLength (full key for ResourceMap, partition key for Partitioned)
			groupIndices := pUtil.calculateGroupAxesIndices(inputKeyLength, aggregationIndices)
			groupKeyLength = len(groupIndices)
			groupElementKeyLength := inputKeyLength - len(groupIndices)
			if groupElementKeyLength < 0 {
				ll.panic("Group element key length is negative: %v", groupElementKeyLength)
			}
			if !isPartitioned && !isJson {
				ll.assert(groupElementKeyLength == len(aggregationIndices),
				  "Group element key length is not equal to aggregation indices length: %v != %v",
				  groupElementKeyLength, len(aggregationIndices))
			}

			if isJson {
				// For PColumnData/Json in aggregation mode
				embeddedData := meta.data
				for sKey, value in embeddedData {
					key := json.decode(sKey) // Parse the JSON-encoded key array

					groupKey := []
					for i in groupIndices {
						if len(key) <= i {
							ll.panic("Key length is less than aggregation index: %v, key: %v", i, key)
						}
						groupKey = append(groupKey, key[i])
					}

					inGroupKey := []
					for i in aggregationIndices {
						if len(key) <= i {
							ll.panic("Key length is less than aggregation index: %v, key: %v", i, key)
						}
						inGroupKey = append(inGroupKey, key[i])
					}

					sGroupKey := json.encode(groupKey)
					sInGroupKey := json.encode(inGroupKey)

					group := groups[sGroupKey]
					if is_undefined(group) {
						// Create a plain map to collect grouped data
						group = {}
						groups[sGroupKey] = group
					}

					// Add the value directly to the group map
					group[sInGroupKey] = value
				}

				// After collecting all data, convert maps to PColumnData/Json resources
				for sGroupKey, groupData in groups {
					groups[sGroupKey] = smart.structBuilder(
						pConstants.RTYPE_P_COLUMN_DATA_JSON,
						json.encode({
							keyLength: groupElementKeyLength,
							data: groupData
						})
					)
				}
			} else {
				// Existing logic for ResourceMap and Partitioned types
				for sKey, field in data.inputs() {
					sKeyP := sKey
					suffix := ""
					if isBinaryPartitioned {
						dotIdx := text.last_index(sKey, ".")
						if dotIdx == -1 {
							ll.panic("BinaryPartitioned key must contain a dot: %v", sKey)
						}
						suffix = sKey[dotIdx:]
						sKeyP = sKey[:(dotIdx)]
					}
					key := json.decode(sKeyP) // Key is the resource key (ResourceMap) or partition key (Partitioned)

					groupKey := []
					for i in groupIndices {
						if len(key) <= i {
							ll.panic("Key length is less than aggregation index: %v, key: %v", i, key)
						}
						groupKey = append(groupKey, key[i])
					}

					inGroupKey := []
					for i in aggregationIndices {
						if len(key) <= i {
							if isPartitioned {
								continue
							} else {
								ll.panic("Key length is less than aggregation index: %v, key: %v", i, key)
							}
						}
						inGroupKey = append(inGroupKey, key[i])
					}

					sGroupKey := json.encode(groupKey)
					// This is the key for the element within the intermediate group ResourceMap
					sInGroupKey := json.encode(inGroupKey)

					group := groups[sGroupKey]
					if is_undefined(group) {
						// Create an intermediate resource to hold aggregated elements.
						// Its type must match the input data type as expected by the body template.
						intermediateGroupType := undefined
						intermediateGroupMetadata := undefined

						if isBinaryPartitioned {
							intermediateGroupType = pConstants.RTYPE_P_COLUMN_DATA_BINARY_PARTITIONED
							intermediateGroupMetadata = { partitionKeyLength: groupElementKeyLength }
						} else if isJsonPartitioned {
							intermediateGroupType = pConstants.RTYPE_P_COLUMN_DATA_JSON_PARTITIONED
							intermediateGroupMetadata = { partitionKeyLength: groupElementKeyLength }
						} else if isParquetPartitioned {
							intermediateGroupType = pConstants.RTYPE_P_COLUMN_DATA_PARQUET_PARTITIONED
							intermediateGroupMetadata = { partitionKeyLength: groupElementKeyLength }
						} else { // isResourceMap (this is the only remaining option)
							intermediateGroupType = pConstants.RTYPE_P_COLUMN_DATA_RESOURCE_MAP
							intermediateGroupMetadata = { keyLength: groupElementKeyLength }
						}

						group = smart.structBuilder(
							intermediateGroupType,
							json.encode(intermediateGroupMetadata)
						)
						groups[sGroupKey] = group
					}
					group.createInputField(string(sInGroupKey) + suffix).set(field)
				}
			}
		}

		outputMaps := {}
		optionalOutputs := {}

		for output in outputs {
			if !is_map(output) {
				ll.panic("malformed output: %v", output)
			}

			reultMightBeNull := false
			for p in output.path {
				if is_map(p) && p.optional {
					reultMightBeNull = true
					break
				}
			}
			if reultMightBeNull {
				optionalOutputs[output.name] = true
			}

			if output.type == "Resource" {
				outputMaps[output.name] = smart.structBuilder(
					pConstants.RTYPE_P_COLUMN_DATA_RESOURCE_MAP,
					json.encode({ keyLength: groupKeyLength })
				)
			} else if output.type == "ResourceMap" {
				outputMaps[output.name] = smart.structBuilder(
					pConstants.RTYPE_P_COLUMN_DATA_RESOURCE_MAP_PARTITIONED,
					json.encode({
						partitionKeyLength: groupKeyLength,
						keyLength: output.keyLength
					})
				)
			} else if output.type == "JsonPartitioned" {
				outputMaps[output.name] = smart.structBuilder(
					pConstants.RTYPE_P_COLUMN_DATA_JSON_SUPER_PARTITIONED,
					json.encode({
						superPartitionKeyLength: groupKeyLength,
						partitionKeyLength: output.partitionKeyLength
					})
				)
			} else if output.type == "BinaryPartitioned" {
				outputMaps[output.name] = smart.structBuilder(
					pConstants.RTYPE_P_COLUMN_DATA_BINARY_SUPER_PARTITIONED,
					json.encode({
						superPartitionKeyLength: groupKeyLength,
						partitionKeyLength: output.partitionKeyLength
					})
				)
			} else if output.type == "ParquetPartitioned" {
				outputMaps[output.name] = smart.structBuilder(
					pConstants.RTYPE_P_COLUMN_DATA_PARQUET_SUPER_PARTITIONED,
					json.encode({
						superPartitionKeyLength: groupKeyLength,
						partitionKeyLength: output.partitionKeyLength
					})
				)
			} else if output.type == "Xsv" {
				// each field will contain super-partitioned data for the corresponding column
				// (specs must be added on the higher level to make a pframe)
				map := {}
				for column in output.settings.columns {
					id := pUtil.xsvColumnId(column)
					map[id] = {} // partKey -> partitioned data resource ref (not-super-partitioned)
				}
				// for regular outputs:
				//   [outputName][groupKey]
				// for xsv output:
				//   [outputName][columnId][groupKey]
				outputMaps[output.name] = map
			} else if output.type == "TsvContent" {
				// ResourceMap to collect TSV content for later processing
				outputMaps[output.name] = smart.structBuilder(
					pConstants.RTYPE_P_COLUMN_DATA_RESOURCE_MAP,
					json.encode({ keyLength: groupKeyLength })
				)
			} else {
				ll.panic("unknonw output type: %v", output.type)
			}
		}

		for sGroupKey, groupValue in groups {
			renderInputs := {}

			if passKey {
				renderInputs[pConstants.KEY_FIELD_NAME] = json.decode(sGroupKey)
			}

			if mappingMode {
				// groupValue here is just a field ref to input map element
				renderInputs[pConstants.VALUE_FIELD_NAME] = groupValue
			} else {
				// in this case groupValue is a resource constructed above
				value := groupValue.lockAndBuild()
				if !is_undefined(anonymizationIndices) && len(anonymizationIndices) > 0 {
					value = anonymize.anonymizePKeys(value, anonymizationIndices).result
				}
				renderInputs[pConstants.VALUE_FIELD_NAME] = value
			}

			for name, field in extraInputs {
				renderInputs[name] = field
			}

			renderResult := render.createUniversal(body, eph, renderInputs, { metaInputs: metaInputs })
			for output in outputs {
				path := output.path
				if is_undefined(path) {
					path = [output.name]
				}

				ref := renderResult.resolveOutput(path)
				reultMightBeNull := optionalOutputs[output.name]

				if output.type == "Xsv" {
					importOps := { dataOnly: true, allowNullInput: reultMightBeNull }

					// Per-output override
					if !is_undefined(output.cpu) {
						importOps.cpu = output.cpu
					} else if !is_undefined(params.cpu) { // Global setting
						importOps.cpu = params.cpu
					}

					if !is_undefined(output.mem) {
						importOps.mem = output.mem
					} else if !is_undefined(params.mem) {
						importOps.mem = params.mem
					}

					if !is_undefined(output.queue) {
						importOps.queue = output.queue
					} else if !is_undefined(params.queue) {
						importOps.queue = params.queue
					}

					if !is_undefined(params.inputCache) && params.inputCache > 0 {
						importOps.inputCache = params.inputCache
					}

					pf := xsv.importFile(ref, output.xsvType, output.settings, importOps)
					for col in output.settings.columns {
						id := pUtil.xsvColumnId(col)
						data := pf.getFutureInputField({name: id, optional: reultMightBeNull})
						outputMaps[output.name][id][sGroupKey] = data
					}
				} else if output.type == "TsvContent" {
					// Store TSV content in ResourceMap for later processing
					applyCache(outputMaps[output.name].
						createInputField(sGroupKey)).
						set(ref)
				} else {
					applyCache(outputMaps[output.name].
						createInputField(sGroupKey)).
						set(ref)
				}
			}
		}

		for output in outputs {
			resultMightBeNull := optionalOutputs[output.name]

			if output.type == "Xsv" {

				partitionKeyLength := 0
				if !is_undefined(output.settings.partitionKeyLength) {
					partitionKeyLength = output.settings.partitionKeyLength
				}

				// if nested xsv import is known to produce unpartitioned output we don't create
				// redundant super-partitioned structures
				unwrapZeroPartition := partitionKeyLength == 0
				// if groupKeyLength == 0 we will directly return the result of xsv.importFile
				// without adding trivial super-partitioning layer
				unwrapZeroGroupKey := groupKeyLength == 0

				dataResType := undefined

				if output.settings.storageFormat == "Binary" {
					if unwrapZeroPartition {
						dataResType = pConstants.RTYPE_P_COLUMN_DATA_BINARY_PARTITIONED
					} else {
						dataResType = pConstants.RTYPE_P_COLUMN_DATA_BINARY_SUPER_PARTITIONED
					}
				} else if output.settings.storageFormat == "Json" {
					if unwrapZeroPartition {
						dataResType = pConstants.RTYPE_P_COLUMN_DATA_JSON_PARTITIONED
					} else {
						dataResType = pConstants.RTYPE_P_COLUMN_DATA_JSON_SUPER_PARTITIONED
					}
				} else if output.settings.storageFormat == "Parquet" {
					if unwrapZeroPartition {
						dataResType = pConstants.RTYPE_P_COLUMN_DATA_PARQUET_PARTITIONED
					} else {
						dataResType = pConstants.RTYPE_P_COLUMN_DATA_PARQUET_SUPER_PARTITIONED
					}
				} else {
					ll.panic("Unknown storage format: %v", output.settings.storageFormat)
				}

				dataResData := undefined
				if unwrapZeroPartition {
					dataResData = json.encode({
						partitionKeyLength: groupKeyLength
					})
				} else {
					dataResData = json.encode({
						superPartitionKeyLength: groupKeyLength,
						partitionKeyLength: partitionKeyLength
					})
				}

				// aggragation column results from all iterations into super-partitioned structures
				// or unwrap them and pass as is if groupKeyLength == 0

				columnResults := {}
				for col in output.settings.columns {
					id := pUtil.xsvColumnId(col)
					parts := outputMaps[output.name][id]
					if unwrapZeroGroupKey {
						ll.assert(len(parts) == 1, "Expected exactly one partition for unwrapping zero group key")
						data := parts[pUtil.EMPTY_JSON_KEY]
						ll.assert(!is_undefined(data), "Expected part \"[]\" to be defined for unwrapping zero group key")
						columnResults[id] = data
					} else {
						dataRes := smart.structBuilder(dataResType, dataResData)
						for sGroupKey, partData in parts {
							if unwrapZeroPartition {
								if output.settings.storageFormat == "Binary" {
									for suffix in [".index", ".values"] {
										applyCache(dataRes.createInputField(sGroupKey + suffix)).set(partData.buildFutureField({name: pUtil.EMPTY_JSON_KEY + suffix, optional: true}))
									}
								} else {
									applyCache(dataRes.createInputField(sGroupKey)).set(partData.buildFutureField({name: pUtil.EMPTY_JSON_KEY, optional: true}))
								}
							} else {
								applyCache(dataRes.createInputField(sGroupKey)).set(partData)
							}
						}
						columnResults[id] = removeNullsLib.removeNulls(dataRes.lockAndBuild())
					}
				}

				if !is_undefined(output.flattenWithDelimiter) {
					for id, data in columnResults {
						outputMaps[output.name + output.flattenWithDelimiter + id] = data
					}
				} else {
					mapBuilder := builder.pFrameBuilder()
					for id, data in columnResults {
						mapBuilder.add(id, undefined, data)
					}
					outputMaps[output.name] = mapBuilder.build()
				}
			} else if output.type == "TsvContent" {
				// Process the collected TSV content with parse-to-json template
				tsvContentResourceMap := outputMaps[output.name].lockAndBuild()

				parseResult := render.create(parseToJsonTpl, {
					params: smart.createJsonResource(output.settings),
					input: tsvContentResourceMap
				})

				parseResultData := parseResult.output("result")

				// Extract individual column results and store them directly
				if !is_undefined(output.flattenWithDelimiter) {
					for column in output.settings.columns {
						columnId := getColumnId(column)
						columnData := parseResultData.buildFutureField({ name: columnId })
						outputMaps[output.name + output.flattenWithDelimiter + columnId] = columnData
					}
				} else {
					outputMaps[output.name] = parseResultData
				}
			} else {
				if resultMightBeNull {
					outputMaps[output.name] = removeNullsLib.removeNulls(outputMaps[output.name].lockAndBuild())
				} else {
					outputMaps[output.name] = outputMaps[output.name].lockAndBuild()
				}
			}
		}

		return outputMaps
	} else {
		ll.panic("Unexpected input resource type: %v", data.info().Type)
	}
})
