ll := import(":ll")
assets := import(":assets")
exec := import(":exec")
validation := import(":validation")
objects := import(":objects")
util := import(":pframes.util")
json := import("json")
constants := import(":pframes.constants")
execConstants := import(":exec.constants")
xsvImportPt := import(":pframes.xsv-import-pt")

transposeXsvSw := assets.importSoftware("@platforma-open/milaboratories.software-small-binaries:table-converter")
pfconvSw := assets.importSoftware("@milaboratories/software-pframes-conv:main")

// import p-frame from directory tpl
importDirTpl := assets.importTemplate(":pframes.import-dir")

renderOps := func(defaults, ...ops) {
	o := { }

	for k, v in defaults {
		o[k] = v
	}

	if len(ops) > 0 {
		for k, v in ops[0] {
			o[k] = v
		}
	}
	return o
}

getAxesSpec := func(spec) {
	axes := []
	for ax in spec.axes {
		axes = append(axes, ax.spec)
	}
	return axes
}

getColumnSpec := func(axesSpec, col) {
	spec := {
		kind: constants.KIND_P_COLUMN,
		axesSpec: axesSpec
	}

	for k, v in col.spec {
		spec[k] = v
	}

	return objects.deleteUndefined(spec)
}

/**
 * Actual implementation of xsv.importFile() (without allowNullInput)
 * Imports xsv data into p-frame. The resulting map resource contains all columns specified in the params (column identifiers as
 * provided by the spec used as map keys). Resulting p-columns will be always single-partitioned at most.
 *
 * @param xsvFile: reference - a reference to a file
 * @param xsvType: string - either csv or tsv
 * @param spec: object - xsv conversion specification
 * @param ops: object - additional options
 *               {
 *                 dataOnly: boolean  - set to true to completely skip creation of specs
 *                 splitDataAndSpec: boolean  - if true, resulting map will have nested structures,
 *                                              with "spec" + "data" fields in nested maps
 *                 transpose: object - if specified, then the input table will be transposed
 *                            {
 *                              separator: string - (optional) use separator instead of inferring from the file extension
 *                              pAxisIdx: index - (optional) the index of the primary axis (column in the input xsv); default 0
 *                              pAxisName: string - (optional) the name of the primary axis (column name in the input xsv); default undefined
 *                              pAxisSearch: regex - (optional) the regex pattern of the name of the primary axis (column name in the input xsv); default undefined
 *                              pAxisNameOverride: string - (optional) override primary axis header in the output; default undefined
 *                              sAxisSearch: string - (optional) regex to filter secondary axis columns in the input table;
 *                              sAxisName: string - (optional) the name of the secondary axis (column name in the output xsv); default "Metric"
 *                              valueName: string - (optional) the name of the value column; default "Value"
 *                              separatorOverride: string - (optional) specify separator in the output (optional)
 *                            }
 *                 cpu: number - (optional) number of cores requested for command.
 *                 mem: number | string - (optional) amount of RAM in bytes or string with size suffix
 *                 queue: string - (optional) the name of the queue. Defaults to the light queue.
 *                 inputCache: duration - (optional) cache duration for execution inputs.
 *               }
 * @return map: reference - a reference to a map resource storing imported data.
 */
importFile := func(xsvFile, xsvType, spec, ...ops) {
	ll.assert(xsvType == "csv" || xsvType == "tsv", "expected one of [tsv, csv] types, found: " + xsvType)

	validation.assertType(spec, util.PFCONV_IMPORT_CFG_SCHEMA)

	// Check that partitionKeyLength is strictly less than the number of axes
	ll.assert(
		spec.partitionKeyLength == undefined || spec.partitionKeyLength < len(spec.axes),
		"partitionKeyLength (%d) must be strictly less than the number of axes (%d)",
		spec.partitionKeyLength,
		len(spec.axes)
	)

	storageFormat := spec.storageFormat
	if is_undefined(storageFormat) {
		storageFormat = "Binary"
	}

	if storageFormat == "Parquet" {
		return xsvImportPt.importFileParquet(xsvFile, xsvType, spec, ops...)
	}

	xsvFileName := "file." + xsvType

	ops := renderOps({ dataOnly: false, splitDataAndSpec: false, queue: execConstants.LIGHT_QUEUE, inputCache: undefined }, ops...)

	if ops.transpose != undefined {

		tops := ops.transpose

		transposedFileName := "fileTransposed." + xsvType

		// convert csv to p-frame and read resulting data
		transposeXsv := exec.builder().
			software(transposeXsvSw).
			printErrStreamToStdout()

		if ops.queue != undefined {
			transposeXsv.setQueue(ops.queue)
		}
		if ops.cpu != undefined {
			transposeXsv.cpu(ops.cpu)
		}
		if ops.mem != undefined {
			transposeXsv.mem(ops.mem)
		}
		if ops.inputCache != undefined && ops.inputCache > 0 {
			transposeXsv.cacheInputs(ops.inputCache)
		}

		if (tops.separator != undefined) {
			transposeXsv = transposeXsv.arg("-input-separator").arg(tops.separator)
		}
		if (tops.pAxisIdx != undefined) {
			transposeXsv = transposeXsv.arg("-sample-column-i").arg("" + tops.pAxisIdx) // always convert to string
		}
		if (tops.pAxisName != undefined) {
			transposeXsv = transposeXsv.arg("-sample-column-name").arg(tops.pAxisName)
		}
		if (tops.pAxisSearch != undefined) {
			transposeXsv = transposeXsv.arg("-sample-column-search").arg(tops.pAxisSearch)
		}
		if (tops.pAxisNameOverride != undefined) {
			transposeXsv = transposeXsv.arg("-sample-label").arg(tops.pAxisNameOverride)
		}
		if (tops.sAxisSearch != undefined) {
			transposeXsv = transposeXsv.arg("-metric-columns-search").arg(tops.sAxisSearch)
		}
		if (tops.sAxisName != undefined) {
			transposeXsv = transposeXsv.arg("-metric-label").arg(tops.sAxisName)
		}
		if (tops.valueName != undefined) {
			transposeXsv = transposeXsv.arg("-value-label").arg(tops.valueName)
		}
		if (tops.separatorOverride != undefined) {
			transposeXsv = transposeXsv.arg("-separator").arg(tops.separatorOverride)
		}

		transposeXsv = transposeXsv.
			arg(xsvFileName).
			arg(transposedFileName).
			addFile(xsvFileName, xsvFile).
			saveFile(transposedFileName).
			run()

		xsvFile = transposeXsv.getFile(transposedFileName)
	}

	// spec without any metadata (for caching purposes)
	pureSpec := util.purifySpec(spec)

	// convert csv to p-frame and read resulting data
	pfconv := exec.builder().
		software(pfconvSw).
		printErrStreamToStdout().
		arg("importCsv").
		arg(xsvFileName).
		arg("-p").arg("spec.json").
		arg("-o").arg("out").
		addFile(xsvFileName, xsvFile).
		writeFile("spec.json", json.encode(pureSpec)).
		processWorkdir("pf", importDirTpl, pureSpec)

	if ops.queue != undefined {
		pfconv.setQueue(ops.queue)
	}
	if ops.cpu != undefined {
		pfconv.cpu(ops.cpu)
	}
	if ops.mem != undefined {
		pfconv.mem(ops.mem)
	}
	if ops.inputCache != undefined && ops.inputCache > 0 {
		pfconv.cacheInputs(ops.inputCache)
	}

	pfconv = pfconv.run()

	// p-columns data
	pf := pfconv.getProcessorResult("pf")
	if ops.dataOnly {
		// @TODO discuss, as it changes the type of the return from map to object
		return pf
	}

	axesSpec := getAxesSpec(spec)

	result := {}
	for col in spec.columns {
		id := util.xsvColumnId(col)

		if ops.splitDataAndSpec {
			result[id] = {
				data: pf.getFutureInputField(id),
				spec: getColumnSpec(axesSpec, col)
			}
		} else {
			result[id + ".data"] = pf.getFutureInputField(id)
			result[id + ".spec"] = getColumnSpec(axesSpec, col)
		}
	}

	return result
}

export ll.toStrict({
	importFile: importFile
})
