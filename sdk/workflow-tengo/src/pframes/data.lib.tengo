ll := import("@platforma-sdk/workflow-tengo:ll")
smart := import("@platforma-sdk/workflow-tengo:smart")
json := import("json")
text := import("text")
// Assuming these utility modules are available as they are used in similar contexts
slices := import("@platforma-sdk/workflow-tengo:slices")
maps := import("@platforma-sdk/workflow-tengo:maps")
pConstants := import("@platforma-sdk/workflow-tengo:pframes.constants")

_createPColumnDataInstance := undefined // Forward declaration

_createDecodeKeyString := func(expectedLength) {
    return func(keyStr) {
        decoded := json.decode(keyStr)
        if !is_array(decoded) {
            ll.panic("Decoded key '%v' from string '%s' is not an array(expected JSON array string).", decoded, keyStr)
        }
        if len(decoded) != expectedLength {
            ll.panic("Decoded key array '%v' (from string '%s') length %d does not match expected top-layer key length %d.", decoded, keyStr, len(decoded), expectedLength)
        }
        return decoded
    }
}

/**
 * parseData creates a new PColumnData "instance" from an input parameter.
 * The input can be a PColumnData resource directly, or a map of the form {data: resource, spec: map}.
 * The returned object directly exposes its fields and methods.
 *
 * @param inputParam The PColumnData resource (e.g., PColumnData/Json) or a map {data: PColumnDataResource, spec: PColumnSpecMap}.
 * @param ...optsRaw Optional map with:
 *   - recursive (bool): If true, recursively parse nested PColumnData for super-partitioned types.
 *   - spec (map): Optional PColumnSpec. If provided, this will override any spec from inputParam.
 *   - parse (bool): If false, return the resource wrapper without particular features (default: true)
 * @returns {object} A PColumnData object with fields and methods.
 */
parseData := func(inputParam, ...optsRaw) {
    opts := {}
    if len(optsRaw) > 0 {
        if len(optsRaw) == 1 && is_map(optsRaw[0]) {
            opts = optsRaw[0]
        } else {
            ll.panic("parseData opts must be a single map argument. Got: %v", optsRaw)
        }
    }

    parse := opts.parse != false
    recursive := opts.recursive == true

    actualResource := undefined
    initialSpec := undefined

    // Check if inputParam is a resource by looking for a typical resource method like .info()
    isReferenceDirectly := smart.isReference(inputParam)

    if !isReferenceDirectly && is_map(inputParam) && !is_undefined(inputParam.data) {
        actualResource = inputParam.data
        if !is_undefined(inputParam.spec) {
            initialSpec = inputParam.spec
        }
    } else if isReferenceDirectly {
        actualResource = inputParam
    } else {
        ll.panic(
            "parseData input must be a resource (a map with an .info() method) or a map of the form {data: resource, spec: map}. Got: %v",
            inputParam
        )
    }

    if parse && !smart.isResource(actualResource) {
        ll.panic("Data must be a valid resource. Got: %v", actualResource)
    }

    state := {
        resource: actualResource,
        resourceType: actualResource.info().Type,
        parsed: parse,
        keyLength: undefined,                 // For PColumnData/Json and PColumnData/ResourceMap
        partitionKeyLength: undefined,        // For partitioned types (top or only level for this instance)
        superPartitionKeyLength: undefined,   // For super-partitioned types
        relevantTopLayerKeyLength: 0,         // Calculated after parsing type
        data: {},                             // The main data store: map[stringKey] -> value | ResourceRef | PColumnDataInstance
        spec: !is_undefined(opts.spec) ? opts.spec : initialSpec
    }

    if !parse {
        return _createPColumnDataInstance(state)
    }

    resourceData := actualResource.getDataAsJson()
    if is_undefined(resourceData) {
         ll.panic("Failed to get JSON data (metadata) from resource of type %s", state.resourceType)
    }

    // Determine relevantTopLayerKeyLength first
    if state.resourceType == pConstants.RTYPE_P_COLUMN_DATA_JSON ||
       state.resourceType == pConstants.RTYPE_P_COLUMN_DATA_RESOURCE_MAP {
        state.keyLength = resourceData.keyLength
        state.relevantTopLayerKeyLength = state.keyLength
    } else if state.resourceType == pConstants.RTYPE_P_COLUMN_DATA_JSON_PARTITIONED ||
              state.resourceType == pConstants.RTYPE_P_COLUMN_DATA_BINARY_PARTITIONED {
        state.partitionKeyLength = resourceData.partitionKeyLength
        state.relevantTopLayerKeyLength = state.partitionKeyLength
    } else if state.resourceType == pConstants.RTYPE_P_COLUMN_DATA_RESOURCE_MAP_PARTITIONED {
        state.partitionKeyLength = resourceData.partitionKeyLength
        state.keyLength = resourceData.keyLength
        state.relevantTopLayerKeyLength = state.partitionKeyLength
    } else if state.resourceType == pConstants.RTYPE_P_COLUMN_DATA_JSON_SUPER_PARTITIONED ||
              state.resourceType == pConstants.RTYPE_P_COLUMN_DATA_BINARY_SUPER_PARTITIONED {
        state.superPartitionKeyLength = resourceData.superPartitionKeyLength
        state.partitionKeyLength = resourceData.partitionKeyLength // This is PKL of *nested* structures
        state.relevantTopLayerKeyLength = state.superPartitionKeyLength
    } else {
        ll.panic("Unsupported PColumn data resource type for initial key length determination: %s", state.resourceType)
    }

    _decodeKeyString := _createDecodeKeyString(state.relevantTopLayerKeyLength)

    if !actualResource.info().InputsLocked {
        ll.panic("Can't parse data resource which inputs are not locked.")
    }

    // Populate state.data
    if state.resourceType == pConstants.RTYPE_P_COLUMN_DATA_JSON {
        state.data = resourceData.data
        for key, _ in state.data { _decodeKeyString(key) }
    } else if state.resourceType == pConstants.RTYPE_P_COLUMN_DATA_JSON_PARTITIONED ||
              state.resourceType == pConstants.RTYPE_P_COLUMN_DATA_RESOURCE_MAP {
        for key, fieldRef in actualResource.inputs() {
            _decodeKeyString(key) // Validate key structure early
            state.data[key] = fieldRef
        }
    } else if state.resourceType == pConstants.RTYPE_P_COLUMN_DATA_BINARY_PARTITIONED {
        tempData := {}
        for key, fieldRef in actualResource.inputs() {
            partKey := undefined
            isIndex := false
            if text.has_suffix(key, ".index") {
                partKey = key[:len(key)-6]
                isIndex = true
            } else if text.has_suffix(key, ".values") {
                partKey = key[:len(key)-7]
            } else {
                ll.warn("Unexpected file in BinaryPartitioned resource: %s", key)
                continue
            }
            _decodeKeyString(partKey) // Validate key structure
            if is_undefined(tempData[partKey]) { tempData[partKey] = {} }
            if isIndex {
                tempData[partKey].index = fieldRef
            } else {
                tempData[partKey].values = fieldRef
            }
        }
        for pk, parts in tempData { // Ensure all parts have both index and values
            if is_undefined(parts.index) || is_undefined(parts.values) {
                ll.panic("Binary partition key %s is incomplete (missing index or values).", pk)
            }
        }
        state.data = tempData
    } else if state.resourceType == pConstants.RTYPE_P_COLUMN_DATA_JSON_SUPER_PARTITIONED ||
              state.resourceType == pConstants.RTYPE_P_COLUMN_DATA_BINARY_SUPER_PARTITIONED ||
              state.resourceType == pConstants.RTYPE_P_COLUMN_DATA_RESOURCE_MAP_PARTITIONED {
        nestedOpts := { parse: recursive } // if not recursive, the nested resource will not be parsed
        if !is_undefined(state.spec) {
            nestedOpts.spec = maps.deepTransform(state.spec, {
                axesSpec: func(axesSpec) {
                    return axesSpec[state.superPartitionKeyLength:]
                }
            });
        }
        for key, fieldRef in actualResource.inputs() {
            _decodeKeyString(key)
            state.data[key] = parseData(recursive ? fieldRef.getValue() : fieldRef, nestedOpts)
        }
    } else if state.resourceType == pConstants.RTYPE_P_COLUMN_DATA_RESOURCE_MAP {
        for key, fieldRef in actualResource.inputs() {
            _decodeKeyString(key)
            state.data[key] = fieldRef
        }
    } else {
        ll.panic("Unsupported resource type for parseData data population: %s", state.resourceType)
    }

    return _createPColumnDataInstance(state)
}

_createPColumnDataInstance = func(state) {
    _decodeKeyString := state.parsed ? _createDecodeKeyString(state.relevantTopLayerKeyLength) : undefined

    self := undefined

    self = ll.toStrict({
        /**
         * The underlying Platforma resource that this PColumnData instance wraps.
         * This can be a resource of various types (PColumnData/Json, PColumnData/JsonPartitioned, etc.)
         * or undefined if the instance was created without an original resource.
         */
        resource: state.resource,

        /**
         * Boolean flag indicating whether this PColumnData instance has been parsed and its data
         * structure populated. When false, most methods will panic if called, except basic property access.
         */
        parsed: state.parsed,

        /**
         * Retrieves the PColumnSpec associated with this PColumnData instance.
         * The spec defines the column's metadata including value type, axes specifications,
         * name, domain, and annotations.
         *
         * @returns {object|undefined} The PColumnSpec object, or undefined if no spec was provided
         */
        getSpec: func() {
            return state.spec
        },

        /**
         * Gets the resource type of the underlying PColumnData resource.
         *
         * @returns {object} Resource type object with Name and Version fields (e.g., {Name: "PColumnData/Json", Version: "1"})
         */
        getResourceType: func() {
            return state.resourceType
        },

        /**
         * The total number of elements in each PColumnKey for non-partitioned types like
         * PColumnData/Json and PColumnData/ResourceMap. Undefined for partitioned types.
         */
        keyLength: state.keyLength,

        /**
         * The number of key elements used for partitioning in partitioned types.
         * For super-partitioned types, this refers to the nested partition key length.
         * Undefined for non-partitioned types.
         */
        partitionKeyLength: state.partitionKeyLength,

        /**
         * The number of key elements used for super-partitioning in super-partitioned types
         * (e.g., PColumnData/Partitioned/JsonPartitioned). Undefined for non-super-partitioned types.
         */
        superPartitionKeyLength: state.superPartitionKeyLength,

        /**
         * The main data store containing the actual column data. Structure varies by resource type:
         * - For Json: map[stringKey] -> value
         * - For ResourceMap: map[stringKey] -> ResourceRef
         * - For Partitioned: map[stringKey] -> ResourceRef or {index: ResourceRef, values: ResourceRef} for binary
         * - For Super-partitioned: map[stringKey] -> PColumnDataInstance
         * Keys are JSON-encoded arrays representing PColumnKeys.
         */
        data: state.data,

        /**
         * Function factory for creating key string decoders with a specific expected length.
         * Returns a function that decodes JSON-encoded key strings into arrays and validates their length.
         *
         * @param {number} expectedLength The expected length of decoded key arrays
         * @returns {function} Decoder function that takes a key string and returns a key array
         */
        parseKey: _createDecodeKeyString,

        /**
         * Extracts all unique values for a specific axis across all keys in the data.
         * Useful for understanding the value domain of a particular axis or for creating
         * filtering options in UI components.
         *
         * @param {number} axisIdx Zero-based index of the axis to extract values from
         * @returns {array} Array of unique values found at the specified axis position
         * @throws Panics if resource is not parsed, axisIdx is invalid, or out of bounds
         */
        uniqueKeyValues: func(axisIdx) {
            if !state.parsed {
                ll.panic("uniqueKeyValues: resource is not parsed.")
            }
            ll.assert(is_int(axisIdx) && axisIdx >= 0, "axisIdx must be a non-negative integer.")
            ll.assert(axisIdx < state.relevantTopLayerKeyLength, "axisIdx %d is out of bounds for the top-layer key structure (length %d).", axisIdx, state.relevantTopLayerKeyLength)

            uniqueValsMap := {}
            for keyStr, _ in state.data {
                keyArray := _decodeKeyString(keyStr)
                val := keyArray[axisIdx]
                uniqueValsMap[string(val)] = val
            }
            return maps.values(uniqueValsMap)
        },

        /**
         * Creates a new Platforma resource from this PColumnData instance's current state.
         * If an original resource exists and the data hasn't been modified, returns the original.
         * Otherwise, constructs a new resource of the appropriate type containing the current data.
         *
         * This method is essential for persisting changes made through filtering, partitioning,
         * or other transformations back into the Platforma resource system.
         *
         * @returns {object} A locked and built Platforma resource ready for use in workflows
         * @throws Panics if resource is not parsed and no original resource exists, or for unsupported resource types
         */
        createDataResource: func() {
            if !is_undefined(state.resource) {
                // if possible, just return the original resource
                return state.resource
            }

            if !state.parsed {
                ll.panic("createDataResource: resource is not parsed and no original resource is set.")
            }

            builder := undefined
            if state.resourceType == pConstants.RTYPE_P_COLUMN_DATA_JSON {
                builder = smart.structBuilder(state.resourceType, json.encode({ keyLength: state.keyLength, data: state.data }))
            } else if state.resourceType == pConstants.RTYPE_P_COLUMN_DATA_JSON_PARTITIONED ||
                    state.resourceType == pConstants.RTYPE_P_COLUMN_DATA_RESOURCE_MAP_PARTITIONED {
                builder = smart.structBuilder(state.resourceType, json.encode({ partitionKeyLength: state.partitionKeyLength }))
                for key, resourceRef in state.data {
                    builder.createInputField(key).set(resourceRef)
                }
            } else if state.resourceType == pConstants.RTYPE_P_COLUMN_DATA_BINARY_PARTITIONED {
                builder = smart.structBuilder(state.resourceType, json.encode({ partitionKeyLength: state.partitionKeyLength }))
                for key, binaryParts in state.data {
                    builder.createInputField(key + ".index").set(binaryParts.index)
                    builder.createInputField(key + ".values").set(binaryParts.values)
                }
            } else if state.resourceType == pConstants.RTYPE_P_COLUMN_DATA_JSON_SUPER_PARTITIONED ||
                    state.resourceType == pConstants.RTYPE_P_COLUMN_DATA_BINARY_SUPER_PARTITIONED ||
                    state.resourceType == pConstants.RTYPE_P_COLUMN_DATA_RESOURCE_MAP_PARTITIONED {
                builder = smart.structBuilder(state.resourceType, json.encode({
                    superPartitionKeyLength: state.superPartitionKeyLength,
                    partitionKeyLength: state.partitionKeyLength
                }))
                for key, item in state.data {
                    builder.createInputField(key).set(item.createDataResource())
                }
            } else if state.resourceType == pConstants.RTYPE_P_COLUMN_DATA_RESOURCE_MAP {
                builder = smart.structBuilder(state.resourceType, json.encode({ keyLength: state.keyLength }))
                for key, resRef in state.data {
                    builder.createInputField(key).set(resRef)
                }
            } else {
                ll.panic("Unsupported resource type for createDataResource: %s", state.resourceType)
            }

            return builder.lockAndBuild()
        },

        /**
         * Filters the PColumnData by fixing specific axis values, effectively slicing the data
         * to include only entries where the specified axes have the given values.
         *
         * The filtering reduces the dimensionality of the data by removing the specified axes
         * from the key structure and only keeping data entries that match the filter criteria.
         * For super-partitioned data, the method can unwrap layers when all super-partition
         * dimensions are filtered away.
         *
         * @param {array} slicingParams Array of [axisIdx, value] tuples specifying which axes to fix.
         *                              Each tuple contains:
         *                              - axisIdx (number): Zero-based index of the axis to filter on
         *                              - value: The specific value to match for that axis
         *                              Example: [[0, "sample1"], [2, "treatment"]] filters axis 0 to "sample1" and axis 2 to "treatment"
         * @returns {object} A new PColumnData instance with the filtered data and reduced key structure.
         *                   If no slicing params provided, returns self unchanged.
         *                   For super-partitioned data where all super-partition axes are filtered,
         *                   may return a non-super-partitioned instance or empty instance.
         * @throws Panics if resource is not parsed, slicing params are invalid, axis indices are out of bounds,
         *         duplicate axis indices are provided, or internal consistency errors occur
         */
        filter: func(slicingParams) {
            if len(slicingParams) == 0 {
                return self
            }

            if !state.parsed {
                ll.panic("filter: resource is not parsed.")
            }

            slicingParams = maps.clone(slicingParams)
            slices.quickSortInPlaceFn(slicingParams, func(a, b) {
                return a[0] < b[0]
            })

            _validateSlicingParamFormat := func(sp) {
                if !is_array(sp) || len(sp) != 2 { ll.panic("Invalid slicing param: %v", sp) }
                if !is_int(sp[0]) || sp[0] < 0 { ll.panic("Invalid axisIdx: %v", sp[0]) }
            }

            fixedAxesGlobal := {}
            for param in slicingParams {
                _validateSlicingParamFormat(param) // Corrected variable name from p to param
                if fixedAxesGlobal[param[0]] { ll.panic("Duplicate axisIdx %d", param[0]) }
                if param[0] >= state.relevantTopLayerKeyLength {
                    ll.panic("Slicing axis %d out of bounds for RTLKL %d", param[0], state.relevantTopLayerKeyLength)
                }
                fixedAxesGlobal[param[0]] = true
            }

            _matchesFilter := func(keyArray) {
                for param in slicingParams {
                    axisIdx := param[0]
                    value := param[1]
                    if keyArray[axisIdx] != value {
                        return false
                    }
                }
                return true
            }

            _transformKey := func(keyArray) {
                newKeyArray := []
                for i, value in keyArray { if !fixedAxesGlobal[i] { newKeyArray = append(newKeyArray, value) } }
                return newKeyArray
            }

            newSpec := undefined

            if !is_undefined(state.spec) {
                newSpec = maps.deepTransform(state.spec, {
                    axesSpec: func(axesSpec) {
                        updatedAxesSpec := []
                        for i, axisSpecEntry in axesSpec { if !fixedAxesGlobal[i] { updatedAxesSpec = append(updatedAxesSpec, axisSpecEntry) } }
                        return updatedAxesSpec
                    }
                })
            }

            if state.resourceType == pConstants.RTYPE_P_COLUMN_DATA_JSON_SUPER_PARTITIONED ||
               state.resourceType == pConstants.RTYPE_P_COLUMN_DATA_BINARY_SUPER_PARTITIONED ||
               state.resourceType == pConstants.RTYPE_P_COLUMN_DATA_RESOURCE_MAP_PARTITIONED {

                newData := {}
                for superKeyStr, item in state.data {
                    superKeyArray := _decodeKeyString(superKeyStr)
                    if _matchesFilter(superKeyArray) {
                        newSuperKeyArray := _transformKey(superKeyArray)
                        newData[string(json.encode(newSuperKeyArray))] = item // Nested item taken as is
                    }
                }

                newSuperPartitionKeyLength := state.superPartitionKeyLength - len(slicingParams)

                if newSuperPartitionKeyLength < 0 {
                    ll.panic("Internal filter error: newSuperPartitionKeyLength is negative.")
                }

                if newSuperPartitionKeyLength == 0 {
                    // All super-partitioning dimensions are sliced away.
                    // The result should be based on the single selected nested item (if any).
                    if len(newData) > 1 {
                        ll.panic("Filter error: too many items (%d) after reducing super-partition key length to 0. Expected 0 or 1.", len(newData))
                    }

                    newInstanceOpts := {
                        parsed: true,
                        spec: newSpec
                    }

                    if len(newData) == 0 {
                        // No matching super-key, return an empty PColumnData of the appropriate *nested* type.
                        newInstanceOpts.data = {}

                        if state.resourceType == pConstants.RTYPE_P_COLUMN_DATA_JSON_SUPER_PARTITIONED {
                            partitionKeyLength := state.partitionKeyLength
                            newInstanceOpts.resourceType = pConstants.RTYPE_P_COLUMN_DATA_JSON_PARTITIONED
                            newInstanceOpts.partitionKeyLength = partitionKeyLength
                            newInstanceOpts.relevantTopLayerKeyLength = partitionKeyLength
                        } else if state.resourceType == pConstants.RTYPE_P_COLUMN_DATA_BINARY_SUPER_PARTITIONED {
                            partitionKeyLength := state.partitionKeyLength
                            newInstanceOpts.resourceType = pConstants.RTYPE_P_COLUMN_DATA_BINARY_PARTITIONED
                            newInstanceOpts.partitionKeyLength = partitionKeyLength
                            newInstanceOpts.relevantTopLayerKeyLength = partitionKeyLength
                        } else { // RTYPE_P_COLUMN_DATA_RESOURCE_MAP_PARTITIONED (PColumnData/Partitioned/ResourceMap)
                            keyLength := state.keyLength
                            newInstanceOpts.resourceType = pConstants.RTYPE_P_COLUMN_DATA_RESOURCE_MAP
                            newInstanceOpts.keyLength = keyLength
                            newInstanceOpts.relevantTopLayerKeyLength = keyLength
                        }

                        return _createPColumnDataInstance(newInstanceOpts)
                    } else { // len(newData) == 1
                        singleNestedItem := newData["[]"] // This is a PColumnData instance
                        if is_undefined(singleNestedItem) || is_undefined(singleNestedItem.getResourceType()) {
                            ll.panic("Filter error: expected a PColumnData instance for key '[]' when unwrapping, got %v", singleNestedItem)
                        }
                        return singleNestedItem
                    }
                } else { // newSuperPartitionKeyLength > 0, still super-partitioned
                    return _createPColumnDataInstance({
                        parsed: true,
                        resourceType: state.resourceType,
                        superPartitionKeyLength: newSuperPartitionKeyLength,
                        partitionKeyLength: state.partitionKeyLength, // This is PKL of nested structures or KL of nested ResourceMap. It doesn't change by slicing super keys.
                        relevantTopLayerKeyLength: newSuperPartitionKeyLength,
                        data: newData,
                        spec: newSpec,
                        isRecursive: state.isRecursive,
                        originalResource: undefined
                    })
                }
            } else if state.resourceType == pConstants.RTYPE_P_COLUMN_DATA_JSON_PARTITIONED ||
                      state.resourceType == pConstants.RTYPE_P_COLUMN_DATA_BINARY_PARTITIONED {
                newData := {}
                for keyStr, item in state.data {
                    keyArray := _decodeKeyString(keyStr)
                    if _matchesFilter(keyArray) {
                        newKeyArray := _transformKey(keyArray)
                        newData[string(json.encode(newKeyArray))] = item
                    }
                }
                newPartitionKeyLength := state.partitionKeyLength - len(slicingParams)
                return _createPColumnDataInstance({
                    parsed: true,
                    resourceType: state.resourceType,
                    partitionKeyLength: newPartitionKeyLength,
                    relevantTopLayerKeyLength: newPartitionKeyLength,
                    data: newData,
                    spec: newSpec,
                    isRecursive: state.isRecursive
                })
            } else if state.resourceType == pConstants.RTYPE_P_COLUMN_DATA_RESOURCE_MAP {
                newData := {}
                for keyStr, item in state.data {
                    keyArray := _decodeKeyString(keyStr)
                    if _matchesFilter(keyArray) {
                        newKeyArray := _transformKey(keyArray)
                        newData[string(json.encode(newKeyArray))] = item
                    }
                }
                newKeyLength := state.keyLength - len(slicingParams)
                return _createPColumnDataInstance({
                    parsed: true,
                    resourceType: state.resourceType,
                    keyLength: newKeyLength,
                    relevantTopLayerKeyLength: newKeyLength,
                    data: newData,
                    spec: newSpec,
                    isRecursive: state.isRecursive
                })
            } else if state.resourceType == pConstants.RTYPE_P_COLUMN_DATA_JSON {
                newData := {}
                for keyStr, val in state.data {
                    keyArray := _decodeKeyString(keyStr)
                    if _matchesFilter(keyArray) {
                        newKeyArray := _transformKey(keyArray)
                        newData[string(json.encode(newKeyArray))] = val
                    }
                }
                newKeyLength := state.keyLength - len(slicingParams)
                return _createPColumnDataInstance({
                    parsed: true,
                    resourceType: state.resourceType,
                    keyLength: newKeyLength,
                    relevantTopLayerKeyLength: newKeyLength,
                    data: newData,
                    spec: newSpec,
                    isRecursive: state.isRecursive,
                    originalResource: undefined
                })
            } else {
                ll.panic("Filtering not supported for type %s", state.resourceType)
            }
        },

        /**
         * Partitions the PColumnData into separate groups based on the unique combinations
         * of values found at the specified axes. Each partition contains only the data entries
         * that share the same values for the partitioning axes.
         *
         * This method is useful for grouping data by certain dimensions, such as partitioning
         * experimental data by sample ID or treatment group. Each resulting partition is a
         * filtered PColumnData instance with the partitioning axes removed from its key structure.
         *
         * @param {...number} axesToPartitionBy Variable number of axis indices to partition by.
         *                                      Each must be a non-negative integer within bounds
         *                                      of the current key structure. Order matters for
         *                                      partition key generation.
         *                                      Example: partition(0, 2) partitions by axes 0 and 2
         * @returns {object} Map where keys are JSON-encoded arrays of the partition values and
         *                   values are PColumnData instances containing the filtered data for that partition.
         *                   Example result: {"[\"sample1\",\"treatment\"]": filteredPColumnDataInstance}
         * @throws Panics if resource is not parsed, no axes provided, axis indices are invalid,
         *         out of bounds, or duplicate axis indices are provided
         */
        partition: func(...axesToPartitionBy) {
            if !state.parsed {
                ll.panic("partition: resource is not parsed.")
            }
            if len(axesToPartitionBy) == 0 {
                ll.panic("partition: at least one axis index must be provided.")
            }

            // Validate axesToPartitionBy
            seenAxesValidation := {}
            for _, axisIdx in axesToPartitionBy {
                if !is_int(axisIdx) || axisIdx < 0 {
                    ll.panic("partition: axis index '%v' must be a non-negative integer.", axisIdx)
                }
                if axisIdx >= state.relevantTopLayerKeyLength {
                    ll.panic("partition: axis index %d is out of bounds for the top-layer key structure (length %d). Current type: %s", axisIdx, state.relevantTopLayerKeyLength, state.resourceType)
                }
                if seenAxesValidation[axisIdx] {
                    ll.panic("partition: duplicate axis index %d provided.", axisIdx)
                }
                seenAxesValidation[axisIdx] = true
            }

            resultPartitions := {}

            for keyStr, _ in state.data {
                keyArray := _decodeKeyString(keyStr)

                currentPartitionTuple := []
                for _, axisIdx in axesToPartitionBy {
                    currentPartitionTuple = append(currentPartitionTuple, keyArray[axisIdx])
                }
                partitionMapKeyStr := string(json.encode(currentPartitionTuple))

                if is_undefined(resultPartitions[partitionMapKeyStr]) {
                    slicingParamsForThisPartition := []
                    for i, axisIdx in axesToPartitionBy {
                        slicingParamsForThisPartition = append(slicingParamsForThisPartition, [axisIdx, currentPartitionTuple[i]])
                    }
                    resultPartitions[partitionMapKeyStr] = self.filter(slicingParamsForThisPartition)
                }
            }
            return resultPartitions
        },

        /**
         * Iterates over all data entries in the PColumnData, calling the provided callback function
         * for each entry with the decoded key array and corresponding value.
         *
         * This method provides a convenient way to process all data entries without needing to
         * manually decode JSON-encoded key strings. The callback receives the key as an array
         * of values corresponding to the axes specification.
         *
         * @param {function} userCallback Function to call for each data entry. Receives two parameters:
         *                                - decodedKey (array): The PColumnKey as an array of axis values
         *                                - value: The data value or resource reference for this key
         *                                Example: func(key, value) { ll.printf("Key: %v, Value: %v", key, value) }
         * @throws Panics if resource is not parsed or userCallback is not a function
         */
        forEach: func(userCallback) {
            if !state.parsed {
                ll.panic("forEach: resource is not parsed.")
            }
            if !is_callable(userCallback) {
                ll.panic("forEach: provided argument is not a function.")
            }

            maps.forEach(state.data, func(keyStr, value) {
                decodedKey := _decodeKeyString(keyStr)
                userCallback(decodedKey, value)
            })
        },

        /**
         * Iterates over partitions of the PColumnData based on specified axes, calling the provided
         * callback function for each partition with the decoded partition key and the filtered PColumnData instance.
         *
         * This method first partitions the data using the specified axes, then iterates over each partition.
         * Each partition contains data entries that share the same values for the partitioning axes,
         * with those axes removed from their key structure.
         *
         * @param {array} axesToPartitionBy Array of axis indices to partition by. Each must be a non-negative
         *                                  integer within bounds of the current key structure.
         *                                  Example: [0, 2] partitions by axes 0 and 2
         * @param {function} userCallback Function to call for each partition. Receives two parameters:
         *                                - partitionKey (array): The partition key as an array of axis values
         *                                - partitionData (object): PColumnData instance containing the filtered data for this partition
         *                                Example: func(partKey, partData) { ll.printf("Partition: %v, Data: %v", partKey, partData) }
         * @throws Panics if resource is not parsed, axesToPartitionBy is not an array, userCallback is not a function,
         *         or any validation errors from the underlying partition method
         */
        forEachPartition: func(axesToPartitionBy, userCallback) {
            if !state.parsed {
                ll.panic("forEachPartition: resource is not parsed.")
            }
            if !is_array(axesToPartitionBy) {
                ll.panic("forEachPartition: first argument must be an array of axis indices.")
            }
            if !is_callable(userCallback) {
                ll.panic("forEachPartition: second argument must be a function.")
            }

            partitions := self.partition(axesToPartitionBy...)

            maps.forEach(partitions, func(partitionKeyStr, partitionPColumnData) {
                decodedPartitionKey := json.decode(partitionKeyStr)
                userCallback(decodedPartitionKey, partitionPColumnData)
            })
        },

        /**
         * Flattens super-partitioned PColumnData by combining the super-partition keys with
         * the nested partition keys to create a single-level partitioned structure.
         *
         * This method is only applicable to super-partitioned resource types:
         * - PColumnData/Partitioned/JsonPartitioned -> PColumnData/JsonPartitioned
         * - PColumnData/Partitioned/BinaryPartitioned -> PColumnData/BinaryPartitioned
         * - PColumnData/Partitioned/ResourceMap -> PColumnData/ResourceMap
         *
         * The resulting structure has a combined key length equal to the sum of the
         * super-partition key length and the nested partition/key length. Each entry's
         * key becomes the concatenation of its super-partition key and nested key.
         *
         * @returns {object} A new PColumnData instance with flattened structure, or self if
         *                   the resource type doesn't support flattening
         * @throws Panics if resource is not parsed, nested instances are not properly structured,
         *         or key length validation fails during flattening
         */
        flatten: func() {
            if !state.parsed {
                ll.panic("flatten: resource is not parsed.")
            }

            isJsonSuper := state.resourceType == pConstants.RTYPE_P_COLUMN_DATA_JSON_SUPER_PARTITIONED
            isBinarySuper := state.resourceType == pConstants.RTYPE_P_COLUMN_DATA_BINARY_SUPER_PARTITIONED
            isResourceMapSuper := state.resourceType == pConstants.RTYPE_P_COLUMN_DATA_RESOURCE_MAP_PARTITIONED

            if !(isJsonSuper || isBinarySuper || isResourceMapSuper) {
                return self // Not a type that can be flattened
            }

            finalResourceType := undefined
            usePartitionKeyLengthForFinal := false
            if isJsonSuper {
                finalResourceType = pConstants.RTYPE_P_COLUMN_DATA_JSON_PARTITIONED
                usePartitionKeyLengthForFinal = true
            } else if isBinarySuper {
                finalResourceType = pConstants.RTYPE_P_COLUMN_DATA_BINARY_PARTITIONED
                usePartitionKeyLengthForFinal = true
            } else { // isResourceMapSuper
                finalResourceType = pConstants.RTYPE_P_COLUMN_DATA_RESOURCE_MAP
                // usePartitionKeyLengthForFinal remains false
            }

            newCombinedKeyLength := -1
            if usePartitionKeyLengthForFinal {
                if is_undefined(state.superPartitionKeyLength) || is_undefined(state.partitionKeyLength) {
                    ll.panic(
                        "Flatten setup error: superPartitionKeyLength (%v) or partitionKeyLength (%v) is undefined for super-type %s. These are needed to determine combined key length.",
                        state.superPartitionKeyLength, state.partitionKeyLength, state.resourceType
                    )
                }
                newCombinedKeyLength = state.superPartitionKeyLength + state.partitionKeyLength
            } else {
                if is_undefined(state.partitionKeyLength) || is_undefined(state.keyLength) {
                    ll.panic(
                        "Flatten setup error: partitionKeyLength (%v) or keyLength (%v) is undefined for super-type %s. These are needed to determine combined key length.",
                        state.partitionKeyLength, state.keyLength, state.resourceType
                    )
                }
                newCombinedKeyLength = state.partitionKeyLength + state.keyLength
            }

            newData := {}

            self.forEach(func(superKeyArray, nestedPCDInstance) {
                if !nestedPCDInstance.parsed {
                    ll.panic("Flatten error: Nested PCD for superKey '%s' (superType %s) is not parsed or missing resourceType.", superKeyArray, state.resourceType)
                }
                if nestedPCDInstance.getResourceType() != finalResourceType {
                    ll.panic("Flatten error: For superKey '%s' (superType %s), expected nested type %s, got %s.",
                        superKeyArray, state.resourceType, finalResourceType, nestedPCDInstance.getResourceType())
                }

                if !usePartitionKeyLengthForFinal { // Nested is ResourceMap
                    if is_undefined(nestedPCDInstance.keyLength) {
                         ll.panic("Flatten error: Nested ResourceMap PCD for superKey '%s' (superType %s) is missing its 'keyLength'.",
                            superKeyArray, state.resourceType)
                    }
                    if nestedPCDInstance.keyLength != state.keyLength {
                        ll.panic("Flatten error: Nested ResourceMap PCD for superKey '%s' (superType %s) has keyLength %d, expected %d.",
                            superKeyArray, state.resourceType, nestedPCDInstance.keyLength, state.keyLength)
                    }
                } else { // Nested is JsonPartitioned or BinaryPartitioned
                    if is_undefined(nestedPCDInstance.partitionKeyLength) {
                         ll.panic("Flatten error: Nested Partitioned PCD for superKey '%s' (superType %s) is missing its 'partitionKeyLength'.",
                            superKeyArray, state.resourceType)
                    }
                    if nestedPCDInstance.partitionKeyLength != state.partitionKeyLength {
                        ll.panic("Flatten error: Nested Partitioned PCD for superKey '%s' (superType %s) has partitionKeyLength %d, expected %d.",
                            superKeyArray, state.resourceType, nestedPCDInstance.partitionKeyLength, state.partitionKeyLength)
                    }
                }

                nestedPCDInstance.forEach(func(nestedKeyArray, valueOrParts) {
                    combinedKeyArray := superKeyArray + nestedKeyArray
                    newData[string(json.encode(combinedKeyArray))] = valueOrParts
                })
            })

            newInstanceOpts := {
                parsed: true,
                resourceType: finalResourceType,
                relevantTopLayerKeyLength: newCombinedKeyLength,
                data: newData,
                spec: state.spec // User's comment update incorporated
            }

            if usePartitionKeyLengthForFinal {
                newInstanceOpts.partitionKeyLength = newCombinedKeyLength
            } else { // For ResourceMap, the length field is 'keyLength'
                newInstanceOpts.keyLength = newCombinedKeyLength
            }

            return _createPColumnDataInstance(newInstanceOpts)
        }
    })

    return self
}

export ll.toStrict({
    parseData: parseData
})
